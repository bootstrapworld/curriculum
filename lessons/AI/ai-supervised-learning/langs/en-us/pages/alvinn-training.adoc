= Supervised Machine Learning: Training ALVINN

== Thinking about Training

@vspace{1ex}

@n @right{@image{../images/forecast.png, 220}}_Refer to the weather forecast (right) in your response._ During a week of test drives, ALVINN steers safely on Friday and Wednesday. On every other day this week, ALVINNâ€™s driving is unsafe and unpredictable. Think about the training corpus of data that prepared ALVINN to produce these results. What was wrong with the training data. How do you know? 

@fitb{}{@ifsoln{ALVINN's best guesses about the steering angle will be accurate when the conditions of the road are similar to the training data.}}

@fitb{}{@ifsoln{Given that ALVINN drove unsafely days when the weather was cloudy, raining, and snowing, we can assume that the training data}}

@fitb{}{@ifsoln{did not include these conditions.}}

@fitb{}{}

@n Imagine that ALVINN has done extensive training on a one-lane road, in all weather conditions, and at all times of day. Would you expect it to be able to safely drive on a busy two-lane road? Explain. @fitb{}{}

@fitb{}{@ifsoln{Although ALVINN's training at this point enables accurate steering in a variety of conditions,}}

@fitb{}{@ifsoln{ALVINN is not trained on busy multi-lane roads. The steering angles differ when the road is wider, and the appearance of}}

@fitb{}{@ifsoln{unexpected, unfamiliar objects (like other cars) could result in very dangerous driving.}}

@n Imagine that a self-driving car trained on isolated country roads, as well as city streets, and highways. What problematic (dangerous) omissions might be left out of its training? @fitb{}{}

@fitb{}{@ifsoln{unexpected moving things like ambulance encounters, bikes, children running into the street to chase a ball, dogs, moose, etc. }}

@fitb{}{@ifsoln{unexpected stationary objects like fallen trees and other debris, hills, flooded roads, street car tracks, bike lanes,}}

@fitb{}{@ifsoln{railroad crossings, draw bridges, etc.}}

== Confidence

@n In addition to producing a steering angle for each image of the road, ALVINN produces a _numeric_ measure of "confidence" in its response.

- What do you think causes ALVINN's "confidence" to increase? @fitb{}{}

@fitb{}{@ifsoln{ALVINN will be more confident when the image of the road matches an image from the training dataset.}}

- What do you think causes ALVINN's "confidence" to decrease? @fitb{}{}

@fitb{}{@ifsoln{ALVINN will be less confident when the image of the road is entirely unlike any image in the training dataset.}}

- Is 100% "confidence" possible? @fitb{}{@ifsoln{100% confidence is not possible, given that ALVINN uses a regression function to determine steering angle }}

@fitb{}{@ifsoln{(rather than checking to see if a test image matches a training image).}}

