= Training Artificial Intelligence

@description{Students consider what training is by exploring two unique examples: song recommendation and plagiarism detection. As a result of this exploration, they learn that training, a resource-intensive and time-intensive process, is the act of transforming data into a model.
}

@ifproglang{pyret}{
@lesson-prereqs{ai-data-driven-algorithms}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

@objectives

| Student-facing Lesson Goals
|

- Let's think about training, the act of transforming data into a model.

| Materials
|[.materials-links]
@material-links

|===


== Song Recommendation Systems


@objective{training}

=== Overview

Students explore song recommendation, another example of data-driven algorithms at work. Students consider how data aggregation is a key component of machine learning.

=== Launch

Data is at the heart of data science, and @vocab{data-driven algorithms} are at the heart of Machine Learning and AI. Providing _more_ data often produces better results, but we must also consider how other variables can influence the quality of output.

@lesson-instruction{
- Read @printable-exercise{case-study-michelle.adoc} and respond to the questions, providing as much detail as you can.
}

@teacher{Invite students to share their responses.

- If their responses highlight that data-driven algorithms produce a higher quality output when we provide more data--great! Your students understood the key take-away from our @lesson-link{ai-data-driven-algorithms} lesson.
- Your students might bring new complexity to the conversation by acknowledging the possibility that a programmer's changes to the algorithm could have caused Michelle's increased satisfaction with her play list.
- If your students do *not* propose that Spotify's algorithm was updated (as we suggest in the answer key), that's okay! *There's no need to reveal that possibility immediately.* We recommend moving on with the lesson. After completing @printable-exercise{song-recommending.adoc}, you can circle back to @printable-exercise{case-study-michelle.adoc}.
}

=== Investigate

For Michelle to get better results, changing the code is not necessarily needed... although it is always a possibility! Let's consider song recommendation in more depth, and explore the idea that sometimes, _there is a need for algorithms to change_.

Very broadly, a song recommendation system does two things:

- aggregate a user's listening history to build a detailed profile of their musical tastes
- given a new song, determine whether or not to recommend that song

What does building the profile for a listener entail?

@lesson-instruction{
- With a partner, complete @printable-exercise{song-recommending.adoc}.
- First, you will think about what data could be collected about a song of your choosing.
- Then, you will (informally) design your own song recommendation system!
}

@slidebreak

@QandA{
@Q{What would your song recommendation system's algorithm prioritize?}
@A{Answers will vary!}
}

@slidebreak

@vocab{Training} is the act of transforming data into a model. As you discovered, song recommendation systems need @vocab{training} in order to make recommendations. In the case of Spotify, the model is a _summary_ of the user's listening habits. Using this model, Spotify can answer questions about new, unseen data (e.g., will Michelle like the latest Taylor Swift song... or not?).

When it comes to song recommendation, sometimes, there is a need for the algorithm to change. When Michelle observed that Spotify must have updated its algorithms, she was *possibly* correct; we actually have no way of knowing what goes on behind the scenes at Spotify. Maybe the algorithm was altered to put more weight on _other_ listeners' behaviors, and less weight on the user's listening behaviors. Or maybe the algorithm was completely overhauled!

@teacher{If your students did not suggest that Spotify improved its algorithms on @printable-exercise{case-study-michelle.adoc}, now is an appropriate time to add some complexity and nuance to the conversation. Discuss the possibility that the algorithm changed a little
or a lot... and that there's no way for us to know!}

=== Synthesize

@QandA{
In machine learning, we generally start with a large chunk of data. A model is then generated from the data. That model is generally expected to be a lot smaller than the original chunk of data (but it may still be huge!). The model can be queried from to get answers.
@Q{Why do you think models are generally smaller than the training data?}

@A{Generally, the model *summarizes* the data, eliminating all but its most essential features--the features that enable it to make predictions, generate text, etc.}
}


== Bags of Words

@objective{bag-of-words}

=== Overview

Students practice thinking like a hacker to determine the basic requirements of a successful plagiarism detection program. They consider the "bag-of-words" model, which a user can query to better understand how similar or different two documents are.

=== Launch

As a student, you probably know what it feels like to be under surveillance.

- When you use the internet at your school or on a school-issued computer, software probably monitors your web use and blocks you from visiting a multitude of sites.
- When you take a test, it's likely proctored.
- When you submit an essay to your English or History teacher, you can expect that they will check for plagiarism - perhaps by running it through a plagiarism detector to be certain that all words and thoughts are your own.
- You might even got to a school where adults are stationed around the building and in the hallways or use cameras to check that students are dressed and behaving a certain way.

Good designers of these systems have to practice adversarial thinking and imagine all the ways that someone might try to _hack_ or _fool_ them.  Let's practice thinking like a designer.

@QandA{

@Q{Imagine that your teacher announces that they will be running all student writing through a plagiarism detector and you are a student who wants to plagiarize. Exercise some creativity: What are your strategies for evading detection?}

@A{Responses will vary, but may include the following: replace common words with synonyms; change the ordering of sentences and paragraphs; plagiarize from an unlikely source (maybe a friend who took the class 5 years ago?); plagiarize from multiple sources; paraphrase text so that it has the correct "voice".}
}

@strategy{Adversarial Thinking}{
Go easy on your students! As students share their plagiarism strategies, you may feel judgmental. We urge you to keep those feelings at bay.

In this exercise, we are trying to get students to engage in *Adversarial Thinking* (put simply, thinking like a hacker). This is a valuable strategy that is taught, for example, in cybersecurity courses at the university level focused on security, data protection, harms caused by AI—these, etc. Adversarial Thinking is a valuable skill for students to develop; the key is that they learn how to exercise it in an ethical way!

Just because students excel at thinking in this way doesn't mean they are ethically compromised. Focus on and commend their creativity and reasoning instead of judging them.
}

To understand the workings of plagiarism detection, we'll start by looking at a simple detector.

@lesson-instruction{
- Open the @starter-file{plagiarism}.
- With a partner, complete @printable-exercise{primitive-plagiarism-detector.adoc}.
}

The `simple-similarity` detector does not work very well! If the plagiarism detector finds a match, we can be certain that an identical document exists. If the detector does not find a match, we know that there are no identical documents. _Either way, we can't draw any conclusions about whether plagiarism happened!_

As we discussed, plagiarizers usually alter at least a few words of the original document. Sometimes they change the ordering of the text, and sometimes they delete a sentence or word here and there. *We need a plagiarism detector with more sophistication!*

=== Investigate

Rather than detecting identicality, we need to determine the _closeness_ of two documents. To do that, we need a way to summarize each document, and then compute the distance between the summaries.

One standard way to summarize a document is by creating a "bag of words" model. Let's try it on two documents (below); each document is an example of jazz "scatting", when a vocalist improvises with nonsense syllables.

- Document a: "doo be doo be doo"
- Document b: "doo doo be doo be"

The bag-of-words summary for Document a looks like this: `"be": 2, "doo": 3`

@lesson-point{
A bag-of-words model represents text as an unordered collection of words with frequencies.
}

As you can see, we've taken the original sentence and disregarded word order, creating a collection that focuses solely on *word frequency*.

@QandA{
@Q{What is the bag-of-words summary for Document b?}
@A{The bag-of-words summary for Document b looks like this: `"be": 2, "doo": 3`.}
@A{It should be identical to the bag-of-words summary for Document a.}
@Q{How did you know what order to put the words in?}
@A{I used the same order as the bag-of-words summary for Document a.}
}

Note: We could have written these bag-of-words summaries as `"doo": 3, "be": 2`, but once we decide on a word order for one document, adhering to that same order is required. The simplest way to be consistent is to use alphabetical order.

The bag-of-words summary for both documents is exactly the same!

A plagiarism detector that uses this model, taking stock of word frequency and ignoring literally everything else, would discover that the two bags of words are a perfect match.


@lesson-instruction{
- Open @starter-file{plagiarism}.
- With a partner, complete @printable-exercise{slightly-less-primitive-detector.adoc}.
}

@QandA{

@Q{How is this plagiarism detector different from our primitive `simple-similarity` plagiarism detector?}
@A{This plagiarism detector compares two bag-of-words summaries, rather than simply comparing two texts.}


@Q{How is this plagiarism detector similar to our primitive `simple-similarity` plagiarism detector?}
@A{Like our primitive plagiarism detector, it checks for identicality. It determines if the two bags of words are identical or not.}
}

Checking if two bags of words are identical *is* an improvement from checking if two texts are identical. That said, this method of plagiarism detection is still pretty ineffective. In the next lesson, we'll consider a major upgrade to our plagiarism detection algorithm.


=== Synthesize

@QandA{

The bag-of-words model is better at detecting plagiarism than the primitive plagiarism detector—but it's far from perfect.

@Q{What kind of plagiarism _can_ we catch using this model?}
@A{We can catch a plagiarizer who reorders the words of a document.}

@Q{What sort of plagiarism are we still _unable_ to catch?}
@A{We cannot catch a plagiarizer who _alters_ the words in a document by substituting in synonyms or changing word tense.}

@Q{What might we _misidentify_ as plagiarism using this model? Put another way, what sort of _non-plagiarism_ might be labeled _plagiarism_?}
@A{Someone might independently write a text with a bag of words that happens to be the same as the bag of words for a different text. This coincidence is more likely with shorter documents. Returning to our Documents a and b: scatting jazz vocalists are not commonly accused of stealing one another's material.}
}


== Vectors and Data Normalization

@objective{data-normalization}

=== Overview

Students explore the importance of data normalization, when we organize data to follow a norm.

=== Launch

Here are some discoveries we have made so far:

- Checking if two texts are identical is not an effective way of detecting plagiarism.
- Summarizing documents as bags of words, and _then_ checking for identicality is better than comparing two texts... but it is also not an effective way of detecting plagiarism.

What we need is a way to check if bags are _similar_! To do this, we will represent our bags as @vocab{vectors}.

- A @vocab{vector} is an ordered list of numbers within parentheses and separated by commas, representing a point.
- Using vector notation, we can represent Document a ("doo be doo be doo") like this: @math{\overrightarrow{a} = (2, 3)}
- If we were to plot a point for the vector on the coordinate plane, it would produce this:

@center{@image{images/3-2.png, 150}}


@QandA{

@Q{How would you represent the vector for Document b ("doo doo be doo be") on the coordinate plane?}

@A{The point would be in the exact same position as the point for Document a. When we plot a point on the coordinate plane, first we plot @math{x} and then we plot @math{y}. There is no such protocol with the bag-of-words model. That said, it is crucial to adhere to the _same word order_ for each bag of words. Because we decided on "doo" then "be" for Document a, we must use "doo" then "be" for Document b also.}
}

=== Investigate

Let's look at some slightly more complicated documents to learn how we can put these @vocab{vectors} to use.

- Document c: "doo be doo be doo doo doo"

- Document d: "be bop bop bop be bop bop"


[cols="1,2,2", options="header", stripes="none"]
|===

| Document
| Bag-of-words summary
| Vector

| c
| `"be": 2, "doo": 5`
| @math{\overrightarrow{c} = (2, 5)}

| d
| `"be": 2, "bop": 5`
| @math{\overrightarrow{d} = (2, 5)}

|===

*We have a problem.*  We can plainly see that Documents c and d are *not* the same ... but their vectors are...

@QandA{
@Q{What went wrong here?}
@A{The point is to draw out student thinking here rather than to get to any particular answer. The remainder of the lesson will dig into the details. Students might suggest:
 - The vectors were written as if there were only two items in the list... but, in fact there are three different items!
 - 5 represents "doo" in the first vector and "bop" in the second vector, but we've lost that information.}
}


@teacher{
*Forgetting to normalize data and consider dimensionality* are common mistakes. Students will discover what these entail during the remainder of the lesson.
}

To solve this problem, let's start by taking a closer look at our data.

First we must recognize that between Documents c and d there are *three* different words. Because there are three words, we need to use a *three* dimensional space, rather than a coordinate plane, which has just two dimensions. We can use a Venn Diagram to visualize the data:

@center{@image{images/scat-venn-diagram.png, 150}}

We must revise our bag-of-words summaries and our vectors!

@teacher{Normalizing data and considering dimensionality requires that--when a word occurs zero times--we acknowledge it. Instead of glossing over the dimension, we indicate that a given word occurred zero times.}

@right{@image{images/2pts.png, 150}}


The new bag-of-words summary for Document c is `"be": 2, "bop": 0, "doo": 5`, which we can represent as  @math{\overrightarrow{c} = (2, 0, 5)}.

The new bag-of-words summary for Document d is `"be": 2, "bop": 5, "doo": 0`, and we can represent it as @math{\overrightarrow{d} = (2, 5, 0)}.

It is a bit trickier to envision plotting these vectors, but not impossible!

@QandA{
@Q{In the 3-dimensional space to the right, which point represents @math{\overrightarrow{c}}? How do you know?}
@A{The one on the bottom. It's at point (2,5) on the be-doo plane, and has moved 0 in the bop direction.}
}

We started out with two documents. Now, in place of our two documents, we have two points that exist at specific locations in a multi-dimensional space.

=== Synthesize


@QandA{

@Q{Earlier in the lesson, you learned that generally, models _summarize_ the data, eliminating all but the most essential features. Which features of the starting document does the bag of words eliminate? Which features does it preserve?}

@A{The bag-of-words model eliminates word order. It preserves word count.}

@Q{Why is it important for the bag-of-words summary to acknowledge when a word occurs zero times?}

@A{Each vector exists in a multi-dimensional space. To compare vectors and consider their closeness, the vectors must exist in the same multi-dimensional space. When we omit a word that occurs zero times, we are in fact omitting a dimension and constructing a broken model.}
}



== The Dimensionality of Natural Language

=== Overview

=== Launch

So far, we've looked at four documents.

- Document a: "doo be doo be doo"
- Document b: "doo doo be doo be"
- Document c: "doo be doo be doo doo doo"
- Document d: "be bop bop bop be bop bop"

Although the documents contain 24 words in total, there are just *_three_* unique words: doo, be, and bop. As a result, we are able to plot these documents as vectors in a *_three_*-dimensional space.

Let's add a fifth document, Document e, to our collection.

- Document e: "doo be bop ski bop bop"

Now we have thirty words total, made up of _four_ unique words: doo, be, bop, and *ski*. Plotting all of our documents would require the use of a _four-dimensional_ space. Having trouble visualizing a four-dimensional space? You're not alone


=== Investigate

A teacher who wants to catch plagiarism will likely opt for a plagiarism detector that has trained on an _extremely_ large collection of documents. A training corpus is a collection of data used to train AI/ML models, enabling them to learn patterns and make prediction. Processing a large @vocab{training corpus} will produce a complex, multi-dimensional model. Every single additional word will add another dimension to the space. Fortunately, computers--unlike humans--have no issue working with multi-dimensional spaces that have hundreds of thousands of dimensions.

@QandA{

@Q{Imagine a plagiarism detector that compares student essays to short strings of jazz vocalizations (such as Documents a-e, that we have worked with in this lesson). Does this comparison seem logical or useful? Explain.}
@A{Totally not useful! It seems very unlikely that a student, assigned to write an essay in academic language, would plagiarize jazz lyrics. Students tend to plagiarize from documents that are at least somewhat connected to the assigned essay topic.}

@Q{What sorts of documents make up the training corpus of an _effective_ plagiarism detector? List as many as you can.}
@A{The corpus would likely include: essays written and submitted by students currently in the class; essays written and submitted by students previously in the class; Wikipedia articles; articles on relevant topics that are available on the internet, etc.}

@Q{Let's say your teacher asks all 20 students in her class to write a 500-word essay. She plans to feed those 20 essays into a plagiarism detector to use as the training corpus, allowing her to detect if two students submitted essays that were a little too similar. *About* how many dimensions will there be in the model?}

@A{Students should provide a wide range of estimates.}
@A{An estimate of 10,000 dimensions (20 essays multiplied by 500 words) is the largest possible estimate here--but it is not necessarily a good estimate. In English, we commonly repeat and reuse words like "the", "and", "a", and so on.}

@A{Other considerations: Did all of the students write about the same topic? How sophisticated is the student writing? Did all students actually write 500 words?}

@A{Taking all of the above into consideration, we can predict that there would probably be at least a few thousand dimensions in the model.}

@Q{What happens if we train on the internet?}
}


@lesson-instruction{
Complete @printable-exercise{human-judgment.adoc} with your partner.}


=== Synthesize

@QandA{

@Q{}

}


== Computing Closeness and Exercising Human Judgment

=== Overview

Students investigate the limitations of plagiarism detection, acknowledging the importance of exercising human judgment.

=== Launch

The training phase is now complete. Let's review what has happened so far.

*1. We created bag-of-words models of our documents.*

In doing so, we compressed the data by isolating the single feature that we care about: word frequency. As a result, the _new_ representation of the data became considerably smaller than the what we started with.

@lesson-point{
Loss of data is a common and often necessary effect of training AI!
}

*2. We normalized our data.*

Comparisons are most useful when we are comparing items that are alike. When building bags of words for the documents in the corpus, each model *must* have the same number of words (dimensions!) regardless of how many words are in a given document. Defaulting to a cliché: we need an "apples-to-apples" comparison, rather than an "apples-to-oranges" comparison. This is why we sometimes need to include words that we did not encounter in a given document in our model.

What now?

=== Investigate

Our primitive plagiarism detector determined if two documents matched perfectly. That plagiarism detector was not especially useful.

Our slightly-less-primitive plagiarism detector determined if two documents' bag-of-words summaries were identical or not... which was also not very useful.

A _more_ effective plagiarism detector will compute the student's vector (a point in a multi-dimensional space), and then compare it to the _other_ points in that space.

To do this, we can use the `cosine-similarity` function.

@strategy{Cosine?!}{

You might be wondering: are we actually using *that* cosine — the one students learn about when studying trigonometry? The answer is YES!

The `cosine-similarity` function computes the cosine of the angle between two vectors. While it is not necessary for students to understand the mathematics happening behind the scenes, the function is a vital part of the program... and a lovely answer to the often-asked question, "When are we ever going to use this?"
}

To allow for a pleasant user experience, a modern plagiarism detector does not actually provide a representation of a multi-dimensional space with varying points. That would be too complicated! Although different plagiarism detectors provide different outputs for their users, here's how the one in Pyret works.

- The `cosine-similarity` function takes in two strings (documents).
- The plagiarism detector produces an output of 1 when the vectors are identical.
- The plagiarism detector produces an output of zero when the vectors are entirely different.
- The plagiarism detector produces a value between zero and 1 for all other comparisons, reflecting the level of similarity of two bags of words.

@lesson-instruction{
- Complete the first section of @printable-exercise{cosine-similarity.adoc}.
}

@teacher{
Invite students to share their responses.}

=== Synthesize

@QandA{

@Q{AI can be impressive... but human judgment is still critical. Why?}

@A{The cosine-similarity function produces a number - and that is all! It is still up to the teacher to decide how to make sense of that number. Over-reliance on programs can result in unfair outcomes.}


@Q{Now that you understand a little bit more about how plagiarism detection programs work, what advice would you offer to a teacher who is considering using one... or to a student who is trying to get away with plagiarism?}
}
