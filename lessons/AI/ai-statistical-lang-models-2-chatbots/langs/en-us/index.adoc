[.beta]
= Statistical Language Modeling: Chatbots

@description{Students consider how statistics and probability drive AI text generation, learning that language models can produce text that sounds credible but may not actually be credible.}

@lesson-prereqs{ai-training-2-language-in-practice}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

@objectives

| Student-facing Lesson Goals
|

- Let's explore how probability influences the output of generative AI models.


| Materials
|[.materials-links]
@material-links

| Preparation
| @preparation{
 
During this lesson, your students will interact with @starter-file{soekia, "Soekia"}, a student-friendly statistical language model. We encourage you to spend plenty of time interacting with @starter-file{soekia, "Soekia"} prior to the lesson. It is a rich learning tool with a lot of interesting features! 
}
|===


== Meet Soekia

=== Overview

Students engage with Soekia to discover some of the nuances of a student-friendly statistical language model.


=== Launch
@slidebreak{Launch}

In @lesson-link{ai-statistical-lang-models-generating-text}, you learned about @vocab{statistical language models} and how they rely on probability to generate text.  You constructed your own language model from the folk song "There Was an Old Lady Who Swallowed a Fly."

In this lesson, you will explore Soekia, a simplified text generation tool designed for student learning. Soekia offers us one example of @vocab{Natural Language Processing}, or using computers to process language. 

@lesson-instruction{
- Open a browser window and make sure it is filling the full width of your screen.
- Go to @starter-file{soekia}.
- The blue-colored panel occupying most of your screen is where text generation takes place. This is the level typically visible to the user when using a chatbot. 
- We're going to tell Soekia to generate a fairy tale. 
- To tell Soekia to start writing, click on the @image{images/send-icon.png,15}.
- Follow along and read the text until Soekia finishes writing.
- When Soekia is done writing, follow the prompts and respond to the questions in the first section of @printable-exercise{meet-soekia-1.adoc}. 
}

@QandA{
@Q{What do you Notice?}
@A{A new word appears about once a second.}
@A{Different words appear in different colors.}
@A{The fairy tale sounds a little strange.}
@A{We can pause, speed up or just add one word at a time using the play controls beneath the text window.}
@A{We can ask Soekia to write another paragraph by clicking "Resume Automatically".}
@A{We can manually select the next word from a list if we want.}
@A{In order to write a new story, we can click on the reset icon @image{/images/reset-icon.png,15} to delete what's been generated.}
@A{We can copy the text that's been generated to our clipboard.}
@A{We can turn color-coding of the words on and off by clicking on @image{images/eye-icon.png,15}.}


@Q{What do you Wonder?}
@A{Why does the story sound so odd?}
@A{Will Soekia tell a different story each time I ask it for a story?}
@A{Does ChatGPT use the same algorithm as Soekia?}
@A{Why are the words highlighted with different colors?}

@Q{Hover your mouse over one or two of the highlighted words in the fairy tale. What appears?
  * If the words in your fairy tale aren't already highlighted with different colors (greens, purples, etc), click the @image{images/eye-icon.png, 15} beneath the generated text.}
@A{Each color corresponds with a different document; documents are labelled A, B, C, etc.}
}

=== Investigate
@slidebreak{Investigate}

It's time to peek behind the curtain to see how a computer can put a statistical language model to use! 

@lesson-instruction{
- Click the "LOOK INSIDE @image{images/arrow-forward-icon.png, 15}" button in the upper right-hand corner of @starter-file{soekia, "Soekia"}.
}

Now, in addition to the blue text generation panel, we see three additional panels: documents, n-grams, and suggested words.

@image{images/four-panels.png}

There is a LOT to discover in each panel! @printable-exercise{meet-soekia-1.adoc} and @printable-exercise{meet-soekia-2.adoc} will prompt you to interact with a few specific features of each panel. After you have a basic sense of each panel's purpose, take a moment to Notice and Wonder. Get curious, click around, and consider how each panel relates to the others that you have already explored.

@lesson-instruction{
Complete the second section (Documents) on @printable-exercise{meet-soekia-1.adoc}.
}

@QandA{

@Q{What did you Notice about the _Documents_ panel?}
@A{There are 12 documents.}
@A{Each document is a different color.}
@A{The colors correspond with the colors that appeared (highlighting words) in the _Text generation_ panel.}
@A{If I click on the icons in the upper right, I can change the training corpus.}
@A{If I change the prompt in the _Text generation_ panel, the green lines at the bottom of the document change. These lines indicate how closely the wording of the prompt corresponds with the text in the document.}

@Q{What did you Wonder about the _Documents_ panel?}
@A{Can I change these documents for different ones?}
@A{Why haven't I heard of some of these fairy tales?}
@A{What is the relationship between this _Documents_ panel and the _N-grams_ panel next to it?}
}

@teacher{Invite students to share their discoveries. Some of your students may discover that they can change the training corpus so that the documents include not fairy tales but tic tac toe games, musical compositions, and chess matches! In the subsequent section of the lesson, students will explore these other document collections in greater depth.}

@lesson-instruction{
Complete the first section (N-grams) on  @printable-exercise{meet-soekia-2.adoc}.
}

@QandA{

@Q{What did you Notice about the _N-grams_ panel?}
@A{All of the n-grams come directly from the documents.}
@A{I can sort the n-grams either alphabetically or by frequency.}
@A{Soekia interprets punctuation marks as words.}
@A{I can tell Soekia to produce unigrams, digrams, trigrams, etc.}

@Q{What did you Wonder about the _N-grams_ panel?}
@A{Why does Soekia interpret a period as a word?}
@A{How does Soekia decide which n-gram it will use in the fairy tale?}
}

@teacher{During @lesson-link{ai-statistical-lang-models-generating-text}, students computed the frequency of different n-grams in smaller training corpus, the song "There Was an Old Lady Who Swallowed a Fly". That is precisely what Soekia is doing in the _N-grams_ panel!}

@lesson-instruction{
Complete the second section (Suggested Words) on  @printable-exercise{meet-soekia-2.adoc}.
}

@QandA{

@Q{What did you Notice about the _Suggested words_ panel?}
@A{The _Suggested words_ panel updates automatically as Soekia generates text.}
@A{Soekia often suggests a 5-gram and the shorter n-grams that are contained within that 5-gram (e.g., "the wind in the trees", "wind in the trees", "in the trees" and "trees").}

@Q{What did you Wonder about the _Suggested words_ panel?}
@A{What does it mean to customize the temperature?}
@A{How does changing the temperature affect the output?}
}


=== Synthesize
@slidebreak{Synthesize}

@QandA{

@Q{Describe in your own words what happens in each of Soekia's inner panels.}
@A{The _Documents_ panel contains the training corpus.}
@A{Soekia processes the documents and produces a list of all possible n-grams (for a given n) in the _N-grams_ panel.}
@A{In the _Suggested words_ panel, Soekia offers possible completions for different inputs.}
@A{The user can set the temperature to choose word suggestions that occur frequently (low temperature) or to suggest words more randomly (high temperature).}
@A{In the _Text Generation_ panel, the output appears automatically or the user can opt to select each word from a a list of suggestions.}



@Q{@vocab{Supervised learning} includes three steps: the demonstration of the learning process, function abstraction, and using the function. Describe what each step includes for the @vocab{supervised learning} of a @vocab{statistical language model}.}
@A{Demonstration: For statistical language models, the demonstration phase is less obvious than in the other cases we have studied (self-driving cars and decision trees). Essentially, a human supervisor is needed to select the documents that form the @vocab{training corpus}. The demonstration that “given an n-gram, the completion is ____” is implicit, not overt. It would be mindbogglingly overwhelming for a human to have to demonstrate each of these completions! There is some degree of self-supervision occurring with this type of model.}
@A{Function abstraction: A statistical language model assigns probabilities to different word sequences, indicating how likely it is that they will occur based on the preceding words (or n-grams).}
@A{Use: Function use involves generating text, one word at a time, based on probability.}
}


== What Makes a Language?

@objective{define-nlp}
@objective{nlp-artificial}

=== Overview

Students discover that statistical language models do not require natural languages to function.

=== Launch
@slidebreak{Launch}

Let's take a break from Soekia for a quick game of tic-tac-toe!

@lesson-instruction{
- Draw a tic-tac-toe grid on your paper and play a game of tic-tac-toe with your neighbor. +
_In case you need a refresher on the game:_
  * The tic-tac-toe board is a 3x3 grid.
  * One person will draw an *X* in one of the squares.
  * The other person will draw and *O* in one of the squares.
  * Keep taking turns - the goal is to get three in a row or block your neighbor from getting three in a row.
  * The game ends when one of you gets three in a row or the grid is full.

Here is a sequence of 5 images showing a game in progress.

@image{images/tic-tac-toe-5-turns.png}

Let's make up a notation to represent this sequence, knowing that:

- the tic-tac-toe grid is a 3x3 coordinate plane with the origin (0,0) in the bottom left corner

- for each move, our notation must indicate: 

  *** the player whose turn it is (X or O)

  *** the ordered pair (x, y) for the location of the player's move on that turn 
}

@QandA{
@Q{How would you annotate the first move in this tic-tac-toe game?}
@A{X22}

@Q{How would you annotate the second move in this tic-tac-toe game?}
@A{O23}

@Q{Annotate the remainder of the moves.}
@A{X12, O13, X33}

@Q{@right{@image{images/docA.png, 50}} Can you translate this "document" written in tic-tac-toe notation into a standard game on a tic-tac-toe board?}

@A{Here is the game played out on a tic-tac-toe board: 

@center{@image{images/tic-tac-toe-solution.png, 75}}
}
}

=== Investigate
@slidebreak{Investigate}

Did you notice that the collection of fairy tales you explored during the first half of this lesson is just one of several available training corpuses? Let's explore some of the others.

@lesson-instruction{
- Open a browser window and make sure it is filling the full width of your screen.
- Follow the directions on @printable-exercise{tic-tac-toe.adoc} to load the Tic-Tac-Toe training corpus in Soekia.
- Complete the first section of @printable-exercise{tic-tac-toe.adoc}
}

Soekia is a great tool for allowing us to look behind the curtain and to watch @vocab{Natural Language Processing} at work. 

Interestingly - as the tic-tac-toe corpus reveals - Natural Language Processing does not actually require a @vocab{natural language}! (A natural language is a language used by humans, like Spanish, English or Swahili.) 

Just like a natural language, the tic-tac-toe text can be parsed into n-grams and then the likelihood of each n-gram's appearance can be determined, so Soekia was able to apply the same algorithms used on our fairytale corpus to produce output.

@QandA{
@Q{Can you think of any other artificial languages that Soekia might be able to process?}
@A{Possible examples: chess moves, musical notation}

@Q{What is required of an artificial language, in order for it to successfully undergo natural language processing?}
@A{It must be broken up with spaces so that it can be interpreted as "words", even if it is not made up of actual words.}
}

@lesson-instruction{
- Follow the directions in the second section of @printable-exercise{tic-tac-toe.adoc} to access the "Music in ABC Notation" training corpus.
- Complete the second section of @printable-exercise{tic-tac-toe.adoc}, "Thinking About Natural Language Processing." 
}

@QandA{
@Q{Does Natural Language Processing require natural language? Explain.}  
@A{No, Natural Language Processing - and statistical language modeling, too - works on artificial languages, such as chess and music notation. As long as the language can be broken into "words" that are separated by spaces, then the text can be processed just like a natural language. The very same algorithms can be applied to a wide variety of languages - both natural and artificial.}
}


=== Synthesize
@slidebreak{Synthesize}

@QandA{
@Q{A student argues that ChatGPT - which was built on the concept of language modeling - is a reliably correct and credible source of information. How would you respond?}
@A{The output that ChatGPT produces depends on the corpus on which it is trained.}
@A{ChatGPT does not actually have any way of assessing for correctness and credibility; it simply produces one output after the next based on a model.}
@A{The very same process that generates so-called "hallucinatory" text _also_ generates the "non-hallucinatory" text.}
@A{The student arguing that ChatGPT is a reliable source of information needs to understand ChatGPT's output _sometimes_ happens to match reality... but sometimes it does not!}
}
