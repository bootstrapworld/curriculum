= Statistical Language Modeling

@description{Students consider how statistics and probability drive AI text generation, learning that language models can produce text that sounds credible but may not actually be credible.}

@lesson-prereqs{ai-training}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

@objectives

| Student-facing Lesson Goals
|

- Let's explore how probability influences the output of generative AI models.


| Materials
|[.materials-links]
@material-links

| Preparation
| @preparation{
- Make a copy of the @online-exercise{https://docs.google.com/forms/u/1/d/1C1WSad_IrVT0pd_IXG7yCCM30CVoIibv9kBDKnEJ5RI/copy, Is it AI? Survey Google Form} so that you can examine your class's data. _If you are using our Google Slides, be sure to add the link to your copy of the form to the appropriate slide._}

|===



== Introduction to Statistical Language Modeling

@objective{predictive-text}

=== Overview

Students explore how text prediction relies on statistical language modeling.

=== Launch

While texting, emailing, or even conducting a web search, you've probably noticed that your thoughts and sentences are sometimes finished for you! Predictive text is now a common feature of many apps, and it is made possible by a certain form of AI: the statistical language model.

@teacher{
Make sure you've made a copy of the @online-exercise{https://docs.google.com/forms/u/1/d/1C1WSad_IrVT0pd_IXG7yCCM30CVoIibv9kBDKnEJ5RI/copy, survey} and have created a link to share with students, so that you can look at your data as a class!
}

@slidebreak

@lesson-instruction{
- Consider this start of a phrase: "Close the..."
- Use the form to record the next word that pops into your mind.
- Type in two other possible next words.
}

@teacher{
Display the survey results for the class to see. If a word is repeated, summarize the results by keeping a tally of how many students suggest each particular word.

Students will think of words like "door", "box", "microwave", "laptop"...
}

=== Investigate

Let's look a little more closely at the words we generated.

@QandA{
@Q{Which words had the _most_ tally marks?}
@Q{Which words had the _least_ tally marks?}
@Q{Are you surprised by the most and least common words? Why or why not?}
@Q{What was our class probability of proposing each of our top three words?}
@A{For example: If there are 25 students in your class, and 8 students chose “door”, the probability of a student choosing that word is 8/25.}
}

@teacher{If your students are comfortable computing the probability of choosing each word, they may do so in pairs or individually. If not, this can be a teacher-led task. Laying a solid foundation is worth the time: students will compute probability throughout the lesson.
}

@slidebreak


@QandA{
@Q{Do you think a text messaging app with a predictive text feature would produce the same results as our class? Why or why not?}
@A{Answers will vary.}
@A{An app probably not suggest all the same words.}
@A{Our class is a pretty small sample, all from the same generation and geography and we are all currently at school so our minds are primed to respond differently than if we were somewhere else.}
}

@teacher{
@right{@image{images/texting-app.png, 200}}Invite conversation. Then share this image from a text messaging app.
}

@ifslide{@image{images/texting-app.png, 200}}

@slidebreak


Similar to other AI systems we've discussed, your predictive text feature is trained on a large amount of data, in this case, text.

During training:

1. The computer extracts each use of the phrase "Close the" from the training dataset, along with the subsequent word.
2. It then determines the probability of each possible next word.
3. When making a suggestion, it offers the words that had the highest probability of following "Close the" in the training dataset.

In this particular @vocab{training corpus}, "door", "doors", and "house"  most commonly appeared after "Close the". As a result, those words are suggested.

@slidebreak

But are all users recommended the _same_ completions?

@QandA{
@Q{What do you think? Will your phone suggest the same words as your classmates' phones would? Why or why not?}
@A{Before we get into the in-depth student-facing explanation, below, invite discussion!}
@A{This question is a great opportunity to revisit previous concepts, in particular the process of training and sampling that students discussed when thinking about @lesson-link{ai-training, "plagiarism detection"}.}
}


@slidebreak

Not all phones will propose the same completions for a variety of reasons:

- Even the same phone won't necessarily propose the same word every time. There might be multiple words with equal probability, in which case the system must use some method of random selection.

- Not all phones use the same system. Systems are generally built with at least a little bit of randomness; the amount of randomness varies from system to system.

- A messaging app also includes a model of the text that _the actual user_ composes. The app therefore offers a degree of personalization based on what the user has chosen in the past. Similarly, the app can take into account the word suggestions that you have previously accepted.

- Some predictive text features even take into account the specific message that you are composing! For instance, typing first letter of an unusual word that you used in the same message triggers the app to propose that unusual word.



@slidebreak

@QandA{
@Q{We just considered four reasons why different phones sometimes propose different completions. Do each of these four reasons represent _statistical_ phenomena? Why or why not?}
@A{Phenomena 1 and 2 are statistical, given that statistical language modeling always includes some element of randomness.}
@A{Phenomena 3 demonstrates the use of a _personalized_ language model, a more refined version of a statistical language model.}
@A{Phenomena 4 is almost anti-statistical! The AI consumes and uses data outside of the model for the corpus.}
}

Phenomena 3 and 4 above suggest that sometimes making a usable tool requires that we step out of bounds! Although pure statistical language models are powerful, the upgrades that programmers develop can make the AI _better_ at completing the task that it was designed to complete.


@slidebreak

You have just considered the workings and in-context use of a @vocab{statistical language model}. Hopefully you have discovered that, although it sometimes may _seem_ like your texting app can read your mind... it can't. It doesn't know the rules of grammar, the meanings of words, or your intentions when you are composing a text. It just knows @vocab{probability}, which it uses in ways that are often very impressive (but sometimes not!).

@teacher{
Throughout the lesson, we'll explore the very important "sometimes not" parenthetical, above.}

=== Synthesize

@QandA{

@Q{Might statistical language modeling be possible for other spoken human languages? Which languages?}
@A{Statistical language modeling will work for any language! The AI does not need to "know" anything about the rules of grammar; it just follows rules that enable it to identify patterns.}

@Q{Can you think of other things besides human spoken languages that a similar approach might work for?}
@A{With statistical language modeling, AI can compose music, play chess games, and more. The "text" need not be made up of words: any symbolic notation at all will do.}
}



== Constructing a Statistical Language Model

@objective{slm}

=== Overview

Students construct a statistical language model by decomposing the text and computing the probabilities of different words appearing.

=== Launch

The best way to make sense of statistical language modeling is to try it yourself! We'll start by constructing a model.

For our corpus, we will use the folk song @handout{old-lady-lyrics.adoc, "There Was an Old Lady Who Swallowed a Fly"}, which tells the nonsensical story of an old lady who swallows a fly, and the unfortunate series of events that follow.

@slidebreak

First, we will decompose the title of our corpus into differently sized chunks (one word at a time, two words at a time, etc.):

[cols="^.^1,^.^1,<.^8", stripes="none", options="header"]
|===

| chunk size | Quantity			| Decomposition

| 1 word
| 9
| (There) (Was) (an) (Old) (Lady) (Who) (Swallowed) (a) (Fly)

| 2 words
| 8
| (There Was) (Was an) (an Old) (Old Lady) (Lady Who) (Who Swallowed) (Swallowed a) (a Fly)

| 3 words
| 7
| (There Was an) (Was an Old) (an Old Lady) (Old Lady Who) (Lady Who Swallowed) (Who Swallowed a) (Swallowed a Fly)

|===

The formal word computer scientists use in this context is not "chunk" but @vocab{n-gram}. In an @math{n}-gram, @math{n} represents the number of words in the chunk. For special cases where @math{n} is 1, 2, or 3, the @math{n}-grams are called unigrams, bigrams, and trigrams.


=== Investigate

Let's dig a little deeper...

@teacher{
Share the @handout{old-lady-lyrics.adoc, song lyrics} with students to read independently. If desired, you could also listen to a recorded version of the song.
}

The phrase "there was an old lady who swallowed a..." is repeated in our corpus! Let's zoom in on one unigram from that phrase: “there”.

@QandA{
@Q{Referring to the @handout{old-lady-lyrics.adoc, "lyrics"}: how many times does the word "there" appear in the song?}
@A{4}
@Q{In this corpus, how many times was the word "there" followed by the word "was"?}
@A{4}
@Q{What is the probability that the word "there" is followed by the word "was"?}
@A{4/4 or 100%}
}


@slidebreak

In the example you just worked through, you computed the probability that "was" appears after the unigram "there" by dividing 4 (how many times we see "was" follow "there") by 4 (how many times we see "there" followed by anything). +

We can represent this computation with a special notation:

@indented{
@math{p(was | there) =}
@math{\frac
	{\textit{count(there was)}}
	{\textit{count(there...)}} = {\frac{4}{4}}}
}

@lesson-instruction{
- Complete @printable-exercise{constructing-model.adoc}.
}



@slidebreak

@QandA{

@Q{What @vocab{training corpus} did you use to construct a language model?}
@A{The song lyrics, including the title of the song, were our corpus.}

@Q{Make a prediction: How can we make use of the ratios we completed on @printable-exercise{constructing-model.adoc}?}
@A{We can refer to our ratios to determine which word is the most likely to follow a given word.}

}

@teacher{
Are you and your students interested in exploring probability in more depth? Check out our lesson on @lesson-link{probability-inference} to dig deeper.
}

=== Synthesize

@QandA{
In our corpus, there were _four_ possible completions for the unigram "the", but there were only _three_ possible completions for the 3-gram "to catch the". +
_In this corpus_, as the n-gram gets longer, the number of completion options decreases.

@Q{Do you think the above statement is true of other corpuses?}
@A{Yes, in general, this is a true statement: longer phrases have fewer possible completions than single words.}
}

== Sampling from the Model

=== Overview

Students use their statistical language model in a generative way, to produce output.

=== Launch

Having built a language model, what can we do with it? We can use it in a generative way: we can produce output!

How might we go about doing that?

- We can start by choosing our first word. A common approach is to ask, "What's the most common @math{n}-gram in the corpus?" but we can also choose the starting word on our own, if we want.
- Next, we ask: "Given the first @math{n}-gram, what is the most common successor?"
- We repeat this second step forever! ...or, more realistically, until we decide to stop the program. A simple statistical language model, however, will generate text ad infinitum.

=== Investigate

Let's give this process a try, returning to our "Old Lady" corpus.

@lesson-instruction{

- Complete the first section of @printable-exercise{sampling.adoc} using @handout{old-lady-lyrics.adoc}.
- Tip: You will be able to work more efficiently if you open the PDF of the handout on a computer and use "Control-F" on a PC or "Command-F" on a Mac to help you locate and count words.
}

@teacher{The two questions below are on students' worksheets, but merit follow-up and discussion.}

@QandA{
@Q{What four-word phrase did you generate?}
@A{"She swallowed a fly"}

@Q{Did everyone in your class end up with same phrase? How and why did that happen?}
@A{Yes. When considering which word to generate next, there was always one word that was clearly the most probable, and there were no ties.}
}


@slidebreak

@lesson-instruction{
- Complete the second section of @printable-exercise{sampling.adoc}.
}

@QandA{
@Q{What four-word phrase did you generate for *Text Generation 2a*?}
@A{The class should be split between "the spider to catch" and "the spider that wriggled".}

@Q{Why didn't everyone end up with the same phrase?}
@A{We were forced to incorporate randomness when there was a tie for the most probable word to follow "spider".}
}


@slidebreak

Modern statistical language models often invite users to adjust the @vocab{temperature} of the generated text, which influences the level of randomness. For instance, ChatGPT users are encouraged to use a _low_ temperature for more focused and less creative tasks. They are encouraged to use a _higher_ temperature for more random and increasingly creative tasks.

@lesson-point{
Temperature is the parameter that controls the randomness of the model's output as it generates text.
}

Even _without_ the ability to raise the temperature, we encountered randomness and variability in our generated texts. With a large enough corpus and a high enough temperature, a statistical language model will produce a new and unique output every single time!

@strategy{AI "Hallucinations"}{

As generative AI produces text, it often generates incorrect or misleading information. This is commonly known as an AI "hallucination".

Some experts dislike this term and are encouraging an end to its use. These experts argue that _all_ output is "hallucinatory". Some of it happens to match reality... and some does not.

The very same process that generates "hallucinatory" text _also_ generates the "non-hallucinatory" text. This truth helps us to understand _why_ it is so difficult to *fix* the "hallucination" problem.

This term also attributes intent and consciousness to the AI, giving it human qualities when it is merely executing a program exactly as it is intended to do.
}

=== Synthesize

@QandA{
Critics of ChatGPT and other language models raise a variety of concerns. Consider each of them, below.

@Q{This form of AI often has trouble with _facts_. Put another way, ChatGPT sometimes "makes stuff up." Why does this happen? What is actually going on?}
@A{When ChatGPT produces false or misleading information, it is not glitching nor is there a bug. ChatGPT is just doing what it does, following the model as it ought to.}

@Q{Others complain that ChatGPT has biases that can be seen in its text output. Where do these biases come from?}
@A{If there are biases in the corpus, there will likely be biases in the output!}
}


== What is the Role of Language?

=== Overview

Students consider whether statistical language modeling requires language.

=== Launch

It's time to peek behind the curtain and see how a computer can put a statistical language model to use! Let's play a quick game to prepare ourselves for making sense of the tool we'll be working with.

@lesson-instruction{
- Draw a tic-tac-toe grid on your paper and play a game of tic-tac-toe with your neighbor. +
_In case you need a refresher on the game:_
  * The tic-tac-toe board is a 3x3 grid.
  * One person will draw and *X* in one of the squares.
  * The other person will draw and *O* in one of the squares.
  * Keep taking turns - the goal is to get three in a row or block your neighbor from getting three in a row.
  * The game ends when one of you gets three in a row or the grid is full.
}

@slidebreak

@lesson-instruction{
@right{@image{images/docA.png, 150}}

- Now draw another tic-tac-toe grid on your paper on which to play the game described by Document A with your neighbor. +
_This info might help you get started:_
  * Think of the tic-tac-toe grid as a 3x3 coordinate plane.
  * Each row of the document contains two pieces of information:
  	*** the player whose turn it is (X or O)
  	*** the ordered pair (x, y) for the location of the player's move on that turn
}

=== Investigate

We're going to explore Soekia, a simplified text generation tool designed for student learning. As we explore, we are going to consider: What is the role of _language_ in a statistical language model?

@lesson-instruction{
- Open a browser window and make sure it is filling the full width of your screen.
- Go to @link{http://bootstrapworld.org/SoekiaGPT/} and click the "LOOK INSIDE" button at the top right of your screen.
- Scroll to the right until the green, right-most panel ("Documents") is in view.
- Click the "Collections" icon (the middle of the three icons in the upper right corner).
- From the drop-down menu that appears, select the "tic-tac-toe" icon (second from the bottom).
- Complete @printable-exercise{tic-tac-toe.adoc}
}

@teacher{
Invite students to share their discoveries. We encourage you to allow students the opportunity to play with Soekia. There are many possible discoveries awaiting your students!
}

=== Synthesize

@QandA{
@Q{Does a statistical language model require language? How do you know?}
@A{No. It just needs some sort of corpus. Natural language is absolutely not needed. Chess games, musical compositions, and tic-tac-toe games are all adequate!}
}


== Deep Dive: Soekia

=== Overview

Students engage with Soekia to discover some of the nuances of statistical language modeling.

=== Launch

Let's dig deeper into Soekia!

@lesson-instruction{
- Go to @link{http://bootstrapworld.org/SoekiaGPT/}
- Complete the first section of @printable-exercise{soekia-intro.adoc}.
- When you're done, let's do a quick survey: Raise your hand if your story was largely inspired by "Felicia and the Pot of Pinks".
}

@teacher{
The vast majority of students will have a story that is primarily sourced from "Felicia and the Pot of Pinks". On the next section of the worksheet, students will discover exactly _why_ this is the case. Feel free to use this mystery as incentive to move on to the next section of the page!
}

@lesson-instruction{
- Complete the second section of @printable-exercise{soekia-intro.adoc}.
}

@QandA{
@Q{Why were so many of our initial stories all about Felicia and the Pot of Pinks?}
@A{The green bar indicates how closely the document matches the box on the "Generate Text" panel. The story "Felicia and the Pot of Pinks" includes the word "tale" once, "fairy" four times, and the word "me" more than a dozen times. With these frequencies, it is a much closer match to "Write me a fairy tale" than any of the other documents.}
}

Let's review what we have done so far:

- We have interacted with Soekia's text generation panel. With modern AI, the text generating interface is the only element that we are privy to. Unlike the AI we use daily, Soekia allowed us to glimpse which words and phrases came from which sources.

- We have also peeked at Soekia's documents panel, or corpus. This is a critical feature of all text-generating AI, but ordinarily, it is hidden from us. Soekia also revealed to us the level of alignment between each document and what _we_ typed into the box on the "Generate Text" panel.

Let's explore the two remaining panels!

@lesson-instruction{
- Turn to @printable-exercise{soekia-closer-look.adoc}.
- Be prepared to share your responses with the class.
}

@ifnotslide{
@teacher{
As students are working, you can share the three tips, below.
}
}

@ifslide{
Advance to the next slide for student-facing tips on navigating Soekia.
}

@slidebreak

If you feel overwhelmed as you work, here are some tips:

- Click "Pause" to review each of the four panels. Ask yourself, "How is _this_ panel related to each of the other panels, in particular, the _adjacent_ panels?"

- Get curious! *Clicking* is powerful. Each time you click, you access previously hidden information. You can click a document, an @math{n}-gram, a suggested word, or even words that appear on the text generation panel.

- To slow down text generation and to allow time to observe changes as they occur, click the "Choose yourself" icon and use your mouse to select words. (You will be prompted to do this in the next activity.)

@teacher{
After they complete the "Closer Look" worksheet, invite students to share what they learned. In particular, have students share their predictions and whether they were correct or not. See if, as a class, you can develop an understanding of any unexpected outcomes.
}

=== Investigate

@lesson-instruction{
- Turn to @printable-exercise{soekia-temperature.adoc}.
- Pause for class discussion once you have completed the first section.
}

As we have discussed @vocab{temperature} is the parameter that controls the randomness of the model's output as it generates text.

@QandA{
@Q{AI sometimes generates false or misleading information. Do you think this is more likely to occur at a high temperature or a low temperature? Explain.}
}

=== Synthesize

@QandA{
@Q{A student argues that AI is a reliably correct and credible source of information. How would you respond?}
@A{The output that AI produces depends on the corpus on which it is trained, but also the language model used to generate the text. The very same process that generates so-called "hallucinatory" text _also_ generates the "non-hallucinatory" text. AI does not actually have any way of assessing for correctness and credibility; it simply produces one output after the next based on a model.}
}
