[.beta]
= Decision Trees

@description{Students explore the technology behind YouTube video recommendations and election outcome predictions. They develop and analyze decision trees to predict individuals’ purchasing decisions.}

@lesson-prereqs{ai-training}


@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...
@objectives

- Create a decision tree from a training dataset.
- Explain contexts in which they might encounter decision trees.


| Student-facing Lesson Goals
|

- Let's create, use, and understand decision trees, the technology behind YouTube recommendations and election outcome predictions

| Materials
|[.materials-links]
@material-links


|===

== What is a Decision Tree? @duration{25 minutes}

=== Overview

Students are introduced to scenarios in which decision trees are used, and then play a game to help them build a frame of reference.

=== Launch

Consider each of three situations described below:

- You just finished watching a YouTube video. Another video appears... and it is exactly the kind of video you would want to watch.
- Two individuals who work for the same company and earn the same salary apply for a loan by filling out the same online form at the local bank. One of them is granted a loan, the other is not.
- A news anchor displays a map with some states colored blue and others colored red. She explains that the map's predictions for the upcoming election were developed using historical census data.

On the surface, these situations may not seem to have a lot in common. If you dig a little deeper, however, you will discover that, in each of these scenarios, a computer uses data to drive the way that it makes recommendations, decisions, and predictions.

@slidebreak

@QandA{
@Q{What kind of data do you think is used for recommending videos?}
@A{Viewing history, search history, channel subscriptions, likes, dislikes, google activity, "not interested" feedback}
@Q{What kind of data do you think is used for classifying loan requests?}
@A{Debt, assets, payment history, number of open accounts, delinquent accounts, income/revenue, expenses, overdrafts in the last 90 days}
@Q{What kind of data do you think is used to predict election outcomes?}
@A{Census data including voter race, gender, income level, party affiliation, age, etc.}
}

@slidebreak

A @vocab{decision tree} is a machine learning algorithm that uses a tree-like model to show decisions and their possible consequences. The scenarios above all use decision trees to arrive at their recommendations.

Decision trees are a form of @vocab{supervised learning}, because the computer is trained on data that already contains the desired categorizations. In other words, the computer trains on example data tagged by a human, and then learns a function that maps from input to output.

=== Investigate

As a class, we are going to play a game to help us understand the sort of thinking that decision trees imitate.

@teacher{The game we are going to play is a simplified version of 20 questions. If students are familiar with 20 questions, discussing the strategies they use when playing the game will help them to make connections.}
@QandA{
@Q{Show of hands: Have you ever played 20 Questions?}
@A{*If the majority of your students haven't played 20 Questions, the discussion questions below are irrelevant; skip straight to the instructions about how our game will work*}
@Q{What are the rules?}
@A{1 player is “it”. They have to think of a secret person, place or thing. The other players can ask up 20 yes or no questions to help them guess the secret person, place, or thing.}
@Q{What strategies do you use when choosing the questions you will ask?}
@A{Ask questions that are likely to eliminate many objects--but be careful! A question that is too narrow could end up resulting in very little information gained.}
@A{Start with questions that will quickly limit possibilities.}
}

@slidebreak

@lesson-instruction{

We're going to play a modified version of that game. Here's how it works:

- In our game there are six possible items: *spoon, knife, cup, plate, fork, and mug*.

- I will write one of them down on a piece of paper.

- You will ask me yes/no questions (however many you need!) until you figure out the object that I chose.

- As you ask questions, I will record them on the board, connecting the questions, answers, followup questions, and their answers with arrows.
}

@teacher{

@right{@image{images/spoon-game.png, 150}}

This activity focuses on showing students the process of building a *decision tree*. The first round of this game will produce a diagram similar to the one on the right, which models a game with the secret word "spoon".  The number of questions required to "guess" the object--most likely between two and four--will depend on what your students ask.

Note that this diagram is *not* a complete decision tree, but a portion of one. The remainder of the tree will grow over the course of the lesson.

Important decision tree conventions:

- Be sure to label arrows with "yes" or "no".

- "Yes" arrows point to the left, and "no" arrows point to the right. 

As students ask questions, support them in noticing how effective their questions are in sorting the items on the list. "Does it have a handle?" split the list between two options that do not have handles (cup, plate) and four that do (spoon, knife, fork, and mug). Other questions could have guaranteed eliminating three choices (half!).

Debrief the activity by posing the questions below. Note that the provided answers correspond with the sample diagram (on the right). Adjust responses to match the diagram that your class produces!
}

@slidebreak

@QandA{

@Q{How many questions did we ask in order to determine the correct object?}
@A{This answer will depend on what your students ask!}
@A{In the example sequence we've diagrammed above, there were 4 questions.}

@Q{Which items from our original list (spoon, knife, cup, plate, fork, and mug) did our very first question eliminate from the list of possibilities?}
@A{This answer will depend on what your students ask!}
@A{In the example sequence we've diagrammed above, there are four items on the list with handles (spoon, knife, fork, and mug). That means our first question--"does it have a handle?" eliminated two items, cup and plate.}

@Q{Can anyone think of a different question that would have eliminated more items right off the bat?}
@A{Responses will vary. "Is it a utensil?" would have split the list in half, given that three items (spoon, knife, and fork) are utensils.}

@Q{How did we decide which questions to ask?}
@A{We had to keep track of which items were eliminated and which items remained in order to pose useful questions.}
}

@slidebreak

Let's play _another_ round of the game with a new item. 

@QandA{
@Q{How many questions did we ask in order to determine the correct object this time?}
@Q{How did we decide which questions to ask?}
@Q{Which items from our original list (spoon, knife, cup, plate, fork, and mug) did our very first question eliminate from the list of possibilities?}
@Q{How are the diagrams we drew similar and how are they different?}
}

@slidebreak

Let's imagine that our first round had started with the question, "Is it a utensil?" and had led us to "knife".  After the first round, our tree might have looked like the diagram on the left (below). If the second round started with the same question, we could have just added to the original diagram... and we might have ended up with something like what you see on the right.

[cols="^.3a,^2a,3a", grid="none", frame="none", stripes="none"]
|===

| @hspace{8em}**Round 1**

|

| @hspace{8em}**Round 2**

| @image{images/tree1.png, 120}

| @image{images/arrow.png, 50}

| @image{images/tree2.png, 370}

|===

@slidebreak

Notice that after Round 2 the topmost question — "is it a utensil?" — splits left ("yes, it is a utensil") *and* right ("no, it is not a utensil").

Our diagram begins with two unique pathways from the top of the tree to two unique items.

Put another way: +
_There are two branches splitting from the tree's root node. Each branch leads to a decision node, which eventually leads to a leaf node._

@slidebreak

@lesson-instruction{
Let's identify the root node, branches, decision nodes, and leaf nodes on our tree so far.
}

@teacher{Discuss your in-progress tree to help students locate the different parts. The parts are intuitively named, but formal definitions are below if needed.}

@vspace{1ex}

@right{@image{images/terminology-tree.png, 300}}

- The *root node* is the very top node that represents the entire population or sample before any splitting occurs.
- @vocab{Splitting} is the process of dividing a node into sub-nodes (decision and/or leaf nodes) with branches. These branches will not necessarily be equal in size.
- *Decision nodes* split from the root node, or from other nodes.
- A *leaf node* is a node that does not split. Just like leaves on most trees, leaf nodes are found at the tip of a branch.

@vspace{1ex}

@slidebreak


@lesson-instruction{
- With a partner, turn to @printable-exercise{decision-tree.adoc} and complete the decision tree so that all six items are categorized.
- Then, draw an entirely different decision tree - one which has a __different__ question at its root.
}

@teacher{

@right{@image{images/tree4.png, 400}}

The completed tree for the first question looks like this.

To create a different tree, students can either choose one of the questions from levels 2 or 3 as their root node, or they can generate an entirely different starting question.

As students finish, invite them to draw their trees on the board. Ideally, you will have a wide assortment of trees! If students are not developing interesting trees, urge them to think of *entirely* different questions than those posed in the sample tree.

}

@slidebreak


@QandA{
@Q{After looking at the decision trees of your classmates: What do these trees all have in common? How are they different?}
@A{Answers will vary. Many trees will have the same number of nodes, although probably not all. Many questions asked will likely be the same, but not every question.}
}

@slidebreak

Let's take a step back and see how well some of our decision trees will perform.

@lesson-instruction{
Complete @printable-exercise{comparing-trees.adoc}.
}

@teacher{
Invite students to share and explain their responses before emphasizing the main ideas, below.
}

@slidebreak


You just observed that a decision tree

- can accurately label and categorize the inputs _that it has been trained to label and categorize_
- falters when offered inputs that are either *unknown* (like chopstick) or *ambiguous* (like spork)

The only way this tree stands a chance of correctly identifying a chopstick or a spork is if we offer it more training!

@QandA{
@Q{Why is it advantageous for AI to be efficient?}
@A{Responses will vary, but may include: reduced delays, an improved user experience, greater scalability, decreased environmental impact.}
@Q{Can you think of any reasons *not* to maximize an AI's efficiency?}
@A{Responses will vary, but students will likely observe that an increase in efficiency leads to a decrease in accuracy.}
}

@slidebreak


As we built our decision trees, we were able to draw on everything we know about every knife, spoon, spork, plate, bowl or mug that we have ever seen. Our decision trees were imperfect because they didn't know about the utensils we either forgot to include or didn't know about.

If you were asked to create a decision tree to identify common animals or foods, you could probably draw on a wealth of knowledge to create a similarly good one without much difficulty.

@slidebreak


Imagine that you are tasked with building a decision tree that can determine an iris' species (_setosa_, _virginica_, and _versicolor_) based on the varying plant measurements. It would be far more challenging to build this tree than it was to build the common tableware classification tree we just made because... !

@centered-image{images/iris-data.png}

@slidebreak


When computers build decision trees, they don't have life experience to draw upon. They only use the data we provide... and that data can sometimes be limited or messy! As a result, we may end up with models that are not 100% accurate.

=== Synthesize

In AI, efficiency and accuracy are often in conflict:

- AI is *efficient* when the computer performs a task with minimal time, memory, energy or data.

- AI is *accurate* when the computer performs its task with correct, relevant, and consistent results.

Striking the perfect balance is an ongoing challenge for computer scientists, and it is a challenge with far-reaching implications.

@QandA{
@Q{Why is it advantageous for AI to be efficient?}
@A{Responses will vary, but may include: reduced delays, an improved user experience, greater scalability, decreased environmental impact.}
@Q{Can you think of any reasons *not* to maximize an AI's efficiency?}
@A{Responses will vary, but students will likely observe that an increase in efficiency leads to a decrease in accuracy.}
}

== Decision Trees from Training Datasets @duration{25 minutes}

=== Overview

Students build a decision tree that predicts whether different individuals will purchase a video game or not.

=== Launch

We have already built some extremely simple decision trees. And we have seen instances where these trees classify successfully as well as instances where they fail.

How do decision trees built from large datasets decide — at every level and every node — which attributes are the most informative ones to ask questions about so that they can make relatively accurate predictions, recommendations, and diagnoses?!

It turns out, there's an algorithm for that, and it's relatively straightforward.


=== Investigate

Have you ever done some online shopping—say, for a new pair of sneakers—only to discover that, for the next several days, you encounter _advertisements for sneakers_ lurking in every corner of the internet that you visit?!

Is it a coincidence?

@slidebreak


No. Websites can store small data files on your device. These "cookies" can be used to remember where you were the last time you visited a site, or a setting that you changed and want to keep the next time you visit a site. One particular kind of cookie, the tracking cookie, allows AI designed for marketing to use your individual browsing habits to decide which ads you will be the most susceptible to.

@slidebreak


We're going to create a decision tree that predicts whether or not different customers at a particular online store will purchase a video game or not. To do so, we must first train the computer! We will use a training dataset that characterizes 14 different shoppers and then indicates whether or not each one purchased a video game.

@QandA{
@Q{With your partner, look over the @handout{decision-tree-data.adoc, Training Dataset}. What do you Notice? What do you wonder?}
@A{Possible responses:}
@A{Individuals in their twenties always buy the video game.} 
@A{There are only three new customers; two out of three times, new customers buy the video game.}
@Q{Can you foresee any problems with making a decision tree based on this dataset? If so, what are they?}
@A{Responses will vary.}
}

@slidebreak


One problem with this dataset is that _age is continuous_. That won’t work!

We need to break these ages down into different groups - which will become different _branches_ that grow out of a _decision node_. *For now, let’s agree to create three groups: teenagers, twenties, and thirties.*

We can use a different representation for this--called a @vocab{decision stump}--than we did with the yes-no questions we saw in our previous decision trees.


@slidebreak


@lesson-instruction{
- We will complete @printable-exercise{level-1.adoc} together, starting with "age" as the root node.
- As we create the first level of our tree, you will discover a tool that we use to complete all the levels of our model: the decision stump.
}

@teacher{
Students are introduced to the @vocab{decision stump} in question 4 on @printable-exercise{level-1.adoc}. Be prepared to guide students through building the decision stump. They will need to use (1) the rule that you wrote in question 1 and (2) the training dataset.}

@slidebreak

A @vocab{decision stump} is a one-level decision tree that makes a prediction based on the value of just a single input feature.

- The stump has three branches because we are considering customers in their teens, twenties and thirties.
- The left-most leaf node ("teens") represents the five teens in our training dataset: Jan (16), Jose (19), Jillian (14), Ariella (16), and Danial (19).
** Jan, Jose, and Jillian did *not* purchase the game, so they are represented by the letter N (for "no").
** Ariella and Danial *did* purchase the game, so they are represented by the letter Y (for "yes").
** We illustrate the teens' decisions with the following shorthand: N N N Y Y
- Finally, we place checkmarks below correctly predicted values to indicate how successful our rule was.
** Our rule predicted that individuals in their teens would *not* purchase the game, so we place checkmarks by the Ns that represent Jan, Jose, and Jillian. Our rule was correct.
** We leave the Ys *without* checkmarks; our rule was wrong for Danial and Ariella.
- We complete the same process for customers in their twenties and in their thirties, representing each individual in the training dataset with a "Y" or "N", and then placing checkmarks to indicate when our rule was successful.

@slidebreak

As we move down the tree, our job is to figure out _which questions_ we should ask and _when_ we should ask them... just like when we play 20 Questions! Decision stumps will help us decide which questions produce a greater information gain.

@strategy{Why Start the Tree with "Age"?}{
Students will likely notice that we seemingly arbitrarily started the tree with "age" as the root node. _Extremely perceptive_ students may notice that for both "age" and "interest", the likelihood of a correct prediction is 10/14. In other words, starting with "interest in game" produces the same information gain as starting with "age" as the root... *so how do we decide?*

It turns out there's more than one correct way to build a decision tree! In general, however, we want to avoid tall, skinny trees that pose one useless question after the other. Rather, it is beneficial to start with an attribute that will result in a _wider_ tree.

Because the "age" node splits _three_ ways and the "interest in game" node splits _two_ ways, we opt to start the tree with "age".
}

@slidebreak

There are two possible questions we could use at the next level of our decision tree :

- Is the individual a frequent customer, an infrequent customer, or a new customer?
- Has the individual expressed interest in a particular video game?

@slidebreak

@lesson-instruction{
- As you complete @printable-exercise{level-2.adoc} you will create and compare different __decision stumps__.
- These "stumps" will help you determine which question will produce the biggest information gain.
- Be ready to share which attributes you plan to add to the second level of your tree.
}

@slidebreak

@lesson-instruction{
- Complete the first section @printable-exercise{build-and-test.adoc}, then let's share the rules we developed.
- "Test the Tree" by completing the second section of @printable-exercise{build-and-test.adoc}.
}

=== Synthesize

@QandA{
@Q{What are some reasons that a decision tree might produce an inaccurate prediction or recommendation?}
@A{If the sample is inconsistent and the prediction represents closer to 50% of the sample population than 100% of the population.}
@A{If the tree has been designed to prioritize efficiency over accuracy, it may produce wrong predictions and recommendations.}
@A{If the training dataset does not accurately represent the broader population, predictions and recommendations will be incorrect.}

@Q{After testing our tree, we discovered that it was not as accurate as we might have presumed. Can you think of any examples of when _missing data_ can create problems?}
@A{Responses will vary.}
@A{When various populations are underrepresented in training datasets, the resulting technology reflects that, and we end up with AI that fails to meet the needs of those populations.}
}



