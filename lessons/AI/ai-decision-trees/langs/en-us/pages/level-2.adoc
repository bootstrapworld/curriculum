= Decision Tree: Level 2

++++
<style>
/* Shrink vertical spacing on fitbruby */
.fitbruby{padding-top: 0.5rem;}
</style>
++++

@link-instructions{Use the @handout{decision-tree-data.adoc, "training data"} to respond to the prompts below.} Let's decide what decision nodes to use in the 2nd level of our tree.

== Shopping History as the Decision Node

@n Let's look at the "shopping history" and "age" columns of the training dataset and see what we can learn about how customer's purchasing habits correspond to their decision to buy.

@indented{
[cols=".^1a, .^2a", stripes="none", grid="none", frame="none"]
|===
| previous customers in their teens:
| @fitbruby{7em}{@ifsoln{1}}{# who bought} out of @fitbruby{7em}{@ifsoln{4}}{# in group} individuals buy the game.

| new customers in their teens:
| @fitbruby{7em}{@ifsoln{1}}{# who bought} out of @fitbruby{7em}{@ifsoln{1}}{# in group} individuals buy the game.
|===
}

@n Let's use what we've learned to write our rule.

@indented{
Predict that
*previous teenage customers* @fitbruby{5em}{@ifsoln{will not}}{will / will not} buy the game and
*new teenage customers* @fitbruby{5em}{@ifsoln{will}}{will / will not} buy the game.
}

@n Create a decision stump for "shopping history" with a root node of "teens". @n Place a checkmark below each value that the computer would predict correctly.


@ifnotsoln{@center{@image{../images/blank-stump.png, 180}}}


@ifsoln{@center{@image{../images/history-stump.png, 180}}}

@n A computer following this rule for our training data would make @fitb{3em}{@ifsoln{4}} correct predictions out of 5 attempts (@fitb{3em}{@ifsoln{80}} % accuracy).

== Interest in Game as the Decision Node

@n Let's look at the "shopping history" and "age" columns of the training dataset and see what we can learn about how customer's purchasing habits correspond to their decision to buy.

@indented{
[cols=".^1a, .^2a", stripes="none", grid="none", frame="none"]
|===
| interested teens:
| @fitbruby{7em}{@ifsoln{2}}{# who bought} out of @fitbruby{7em}{@ifsoln{2}}{# in group} individuals buy the game.
| uninterested teens
| @fitbruby{7em}{@ifsoln{0}}{# who bought} out of @fitbruby{7em}{@ifsoln{3}}{# ingroup} individuals buy the game.
|===
}

@n Let's use what we've learned to write our rule.

@indented{
Predict that
*interested teenage shoppers* @fitbruby{5em}{@ifsoln{will}}{will / will not} buy the game and
*uninterested teenage shoppers* @fitbruby{5em}{@ifsoln{will not}}{will / will not} buy the game.
}

@n Create a decision stump for teenagers' interest in the game. Place a checkmark below each value that the computer would predict correctly.


@ifnotsoln{@center{@image{../images/blank-stump.png, 180}}}

@ifsoln{@center{@image{../images/interest-stump.png, 180}}}


@n A computer following this rule for our training data would make @fitb{3em}{@ifsoln{5}} correct predictions out of 5 attempts (@fitb{3em}{@ifsoln{100}} % accuracy).

== What decision attribute should we use?

@n Will you use shopping history or interest for your second decision node? Explain your response. @fitb{}{@ifsoln{Interest in game. }}

@fitb{}{@ifsoln{When we use this decision attribute, our rule is correct 100% of the time; when we use shopping history, the rule is correct 80% of the time.}}
