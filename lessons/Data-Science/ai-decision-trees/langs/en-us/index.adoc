= Decision Trees

@description{Students }

@ifproglang{pyret}{
@lesson-prereqs{ds-intro}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...



| Student-facing Lesson Goals
|


| Materials
|[.materials-links]
@material-links


|===

== What is a Decision Tree? @duration{10 minutes}

=== Overview

=== Launch

Consider each of three situations described below:

- You just finished watching a youtube video. Another video appears... and it is exactly the video you never knew you wanted to watch.
- Two individuals who work for the same company and earn the same salary apply for a loan from the same bank. One of them is granted a loan, the other is not.
- A news anchor displays a map with some states colored blue and others colored red. She explains that the map's predictions for the upcoming election were developed using census data.

On the surface, these situations may not seem to have a lot in common. If you dig a little deeper, however, you will discover that machine learning is at work in all three. In each of these scenarios, a computer uses data to drive the way that it makes recommendations, decisions, and predictions.

@QandA{
Think about those three scenarios: recommending videos, classifying loan requests, and predicting election outcomes. What sort of data do you think was used in each?
}

A @vocab{decision tree} is a type of machine learning that categorizes and makes predictions based on how a previous set of questions was answered. The scenarios above all use decision trees to arrive at their recommendations. Decision trees are a form of _supervised_ machine learning because the computer is trained on data that contains the desired categorizations.



=== Investigate

Decision trees pop up everywhere because they are flexible and easy to interpret—and they are flexible and easy to interpret because they function similarly to human thinking and decision-making.

As a class, we are going to play a _simplified_ version of 20 Questions to help us understand the sort of thinking that decision trees imitate.

@QandA{
@Q{Show of hands: Have you ever played 20 Questions?}
@Q{What are the rules?}
@A{Choose 1 player to be “it,” and have them think of a secret person, place or thing. The other players attempt to guess the secret person, place, or thing, but they only get 20 questions to do so.}
@Q{Have you ever won 20 Questions? What do you think is the secret to success?}
@A{Possible responses: Ask questions with responses that can definitely be responded to with "yes" or "no". Ask questions that are likely to eliminate many objects... but be careful! A question that is too narrow could end up resulting in very little information gained. Start with questions that will quickly limit possibilities.}
}

@lesson-instruction{

Here's how our game works:

- I will secretly write down on a piece of paper one of the six items on this list: *spoon, knife, cup, plate, fork, and mug*.

- You will ask me yes/no questions (however many you need!) until you figure out the object that I chose.

- As you ask questions, I will record them on the whiteboard connected by arrows.
}

@teacher{

The focus of this activity is to show students the process of building a *decision tree*. The diagram on the right models a 20 questions game with the secret word "knife". Note that this diagram is *not* a complete decision tree, but a portion of one. The remainder of the tree grows over the course of the lesson.

If you want your tree to look similar to the sample, choose "knife" as the first secret word, then help students to notice that there are three utensils on the list of items.  There is no need for your class diagram to be an exact copy of the sample provided.

@right{@image{images/tree1.png, 200}}


The first round of this game will produce a diagram similar to the one on the right. Your diagram will probably have two or three arrows, depending on the questions your students pose.

Important decision tree conventions:

- Be sure to label arrows with "yes" or "no".

- "Yes" arrows point to the left, and "no" arrows point to the right. (The chain of arrows in the diagram is pointing 45 degrees to the left because both questions were answered "yes".)

}

@QandA{
@Q{How many questions did we ask in order to determine the correct object?}
@Q{How did we decide which questions to ask?}
@Q{Which items did our very first question eliminate from the list of possibilities?}
@A{Responses will vary. If students asked, "Is it a utensil?", then the eliminated items would include: plate, bowl, and mug.}
}

Let's play _another_ round of the game - but this time, you will try to guess one of the items that we eliminated by asking our original question. As you ask questions, I will add to our previous diagram.

@teacher{

@right{@image{images/tree2.png, 300}}

As students ask questions, add to your diagram.

Round 2 of this game will produce a diagram similar to the one on the right. Notice that the topmost question—"is it a utensil?"—now splits left ("yes, it is a utensil") *and* right ("no, it is not a utensil").

Be sure to always label arrows, as this is a standard convention and improves readability.

}

So far, our diagram offers two unique pathways from the top of the tree to two unique items.

Put another way: __There are two branches that split from the tree's root. Each branch leads to a decision node, which eventually leads to a terminal node (or "leaf").__

@lesson-instruction{
Let's identify the root, branches, nodes, and leaves on our tree so far.
}

@teacher{Annotate your in-progress tree to help students locate the different parts. The parts are intuitively named, but formal definitions are below if needed.}

@right{@image{images/tree-structure-stolen.png, 300}}

- The root node is the very top node that represents the entire population or sample.
- Splitting is the process of dividing a node into sub-nodes with branches.
- Sub-nodes (or decision nodes) split from the root node, or from other nodes.
- A leaf node (or terminal node) is a node that does not split.

@lesson-instruction{
- With a partner, complete the decision tree on a piece of paper so that all six items are categorized.
- Now, draw an entirely different decision tree - one which has a __different__ question at its root.
}

@teacher{

@right{@image{images/tree3.png, 350}}

One possible completed tree is on the right.

To create a different tree, students can start with any of the questions from levels 2 or 3, or they can generate an entirely different starting question.

As students finish, invite them to draw their trees on the board. Ideally, you will have a wide assortment of trees! If students are not developing interesting trees, urge them to think of *entirely* different questions than those posed in the sample tree.

}

@QandA{
@Q{After looking at the decision trees of your classmates: What do these trees all have in common? How are they different?}
@A{Answers will vary. Each tree will have twice as many branches as nodes. Many trees will have the same number of nodes, although probably not all. Many questions asked will likely be the same, but not every question.}
@Q{Are all of the trees equally efficient?}
@A{The trees are probably similarly efficient, requiring either five or six nodes, including the root. Five nodes is more efficient than six.}
}

You might be wondering: Would it be possible to make this tree any more efficient? If so, why not?!

Let's think about and test two possible trees: the one on the left is more efficient, and the one on the right is less efficient.


@QandA{
@Q{What do you Notice and Wonder about these decision trees?}
@A{Possible response: They are different because the "not flat" branch ends on the efficient tree terminates with "mug or cup".}
@Q{Are the two trees equally accurate? Explain.}
@A{The tree with four nodes cannot label the six listed items with 100% accuracy. It cannot distinguish between "cup" and "mug".}
}

As we increase the tree's efficiency, we can lose accuracy. Conversely, if we focus *too* much on accuracy - making our decision trees bigger and more complex - we risk __overfitting__. Overfitting happens when we teach a model the specific quirks of one particular dataset, preventing it from making reliable predictions about new data.

@QandA{
@Q{How will the decision tree on the right label the following items: (a) knife, (b) chopstick, and (c) spork?}
@A{The tree will label a knife as a knife; it will not know what to do with a chopstick... maybe it's a mug?; and it will call a spork a fork!}
}

Decision trees can accurately label and categorize the inputs that they are trained to label and categorize! This tree—which was 100% accurate on the trained dataset—falters when we offer it inputs that are either *unknown*, like the chopstick, or *ambiguous*, like the spork. In both instances, the tree offers up its best guess at a label. __The only way this tree stands a chance of correctly identifying a chopstick or a spork is if we offer it more training!__


=== Synthesize


As we built our decision trees, we were able to draw on everything we know about every knife, spoon, spork, plate, bowl or mug that we have ever seen. If you were asked to create a decision tree to identify common animals or foods, you could probably do that without much difficulty as well.

Computers, however, build decision trees using only the data we provide... and that data can sometimes be messy. As a result, we may end up with models that are not 100% accurate.

Imagine that you are tasked with building a decision tree that can determine an iris' species (_setosa_, _virginica_, and _versicolor_) based on the varying plant measurements. It would be far more challenging to build this tree than the one we just made that classifies common tableware!

@center{@image{images/iris-data.png}}

(British statistician and biologist Ronald Fisher first published his findings about the Iris Dataset in 1936. The dataset includes 150 different plants' sepal length, sepal width, petal length, and petal width. Today, this dataset is considered a go-to example dataset useful for illustrating a wide range of problems in data science and machine learning.)


== Decision Trees from Training Datasets

=== Launch

We have already built some extremely simple decision trees. We have a sense of the heirarchical structure, flexibility, and versatility of decision trees. We know that decision trees sometimes fail when provided with ambiguous or unknown data. We understand the risk of overfitting our data (building a model that can only succeed on narrow set of data).

We have *not* yet considered how useful, powerful decision trees built from large datasets can make relatively accurate predictions, recommendations, and diagnoses.

The key to building an effective decision tree is to decide—at every level and every node—which attributes are the most informative ones to ask questions about.

...but how do we make such decisions?!

It turns out, there's an algorithm for that, and it's relatively straightforward.


=== Investigate

Have you ever done some online shopping—say, for a new pair of sneakers—only to discover that, for the next several days, you encounter _advertisements for sneakers_ lurking in every corner of the internet that you visit?!

Is it a coincidence? No. Computer cookies are small data files stored locally on your device. Tracking cookies allow marketers to examine your individual browsing habits... and use them to target you based on your interests.

We're going to create a decision tree that predicts whether or not different customers at a particular online store will purchase a video game or not.

To do so, we will train the computer using a dataset (below) that characterizes 14 different shoppers using a number of attributes, and then indicates whether each purchased a video game. (This dataset is also available on @handout{decision-tree-data.adoc}).

@QandA{
@Q{What do you Notice about the @handout{decision-tree-data.adoc}? What do you wonder?}
@Q{Can you foresee any problems with making a decision tree based on this dataset? If so, what are they?}
@A{Responses will vary.}
}

One problem with this dataset is that _age is continuous_. That won’t work! We need to break these ages down into two different groups - which will become two different _branches_ that grow out of a _decision node_. *For now, let’s agree to create three groups: teenagers; twenties; and thirties.*

There are a few possible questions we could use at the root of our decision tree :

- Is the individual in their teens, twenties, or thirties?
- Is the individual a frequent customer, an infrequent customer, or a new customer?
- Is the individual a gamer?
- Has the individual expressed interest in a particular video game?

It’s our job to figure out which question we should ask first. But how?

@lesson-instruction{
- As you complete @printable-exercise{which-question.adoc} you will create different __decision stumps__.
- When you are finished with @printable-exercise{which-question.adoc}, you will be ready to build the most efficient and accurate decision tree possible!
- Be prepared to share with the class which decision attribute belongs at the root of the tree.
}

@teacher{
Once students have finished, invite them to share which attribute they selected for the tree's root node. Have a few students defend their decision.
}

Great! We know how to *begin* our tree. Let's build the rest, then test it to see if our tree makes accurate and useful predictions.

@lesson-instruction{
- Complete the first section @printable-exercise{build-and-test.adoc}, then let's share the rules we developed.
- "Test the Tree" by completing the second section of @printable-exercise{build-and-test.adoc}.
- Get some additional practice with decision stumps on the third section of the page.
}

=== Synthesize

- synthesize Q1
- synthesize Q2
- synthesize Q3
