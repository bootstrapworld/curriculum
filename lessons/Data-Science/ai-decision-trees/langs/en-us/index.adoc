= Decision Trees

@description{Students play a modified version of the game "20 Questions" to explore how decision trees work, before building their own decision trees from training datasets.}

@ifproglang{pyret}{
@lesson-prereqs{ds-intro}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

- Create a decision tree from a training dataset.
- Explain contexts in which they might encounter decision tree AI.


| Student-facing Lesson Goals
|

Let's create, use, and understand decision trees!

| Materials
|[.materials-links]
@material-links


|===

== What is a Decision Tree? @duration{25 minutes}

=== Overview

Students explore scenarios in which decision trees are used, and then play a modified game of 20 Questions to consider the mechanics of decision trees.

=== Launch

Consider each of three situations described below:

- You just finished watching a youtube video. Another video appears... and it is exactly the video you never knew you wanted to watch.
- Two individuals who work for the same company and earn the same salary apply for a loan from the same bank. One of them is granted a loan, the other is not.
- A news anchor displays a map with some states colored blue and others colored red. She explains that the map's predictions for the upcoming election were developed using census data.

On the surface, these situations may not seem to have a lot in common. If you dig a little deeper, however, you will discover that machine learning is at work in all three. In each of these scenarios, a computer uses data to drive the way that it makes recommendations, decisions, and predictions.

@QandA{
Think about those three scenarios: recommending videos, classifying loan requests, and predicting election outcomes. What sort of data do you think was used in each?
}

A @vocab{decision tree} is a type of machine learning that categorizes and makes predictions based on how a previous set of questions was answered. The scenarios above all use decision trees to arrive at their recommendations. Decision trees are a form of _supervised_ machine learning because the computer is trained on data that contains the desired categorizations.

=== Investigate

Decision trees pop up everywhere because they are flexible and easy to interpret—and they are flexible and easy to interpret because they function similarly to human thinking and decision-making.

As a class, we are going to play a _simplified_ version of 20 Questions to help us understand the sort of thinking that decision trees imitate.

@QandA{
@Q{Show of hands: Have you ever played 20 Questions?}
@Q{What are the rules?}
@A{Choose 1 player to be “it,” and have them think of a secret person, place or thing. The other players attempt to guess the secret person, place, or thing, but they only get 20 questions to do so.}
@Q{Have you ever won 20 Questions? What do you think is the secret to success?}
@A{Possible responses: Ask questions with responses that can definitely be responded to with "yes" or "no". Ask questions that are likely to eliminate many objects... but be careful! A question that is too narrow could end up resulting in very little information gained. Start with questions that will quickly limit possibilities.}
}

@lesson-instruction{

Here's how our game works:

- I will secretly write down on a piece of paper one of the six items on this list: *spoon, knife, cup, plate, fork, and mug*.

- You will ask me yes/no questions (however many you need!) until you figure out the object that I chose.

- As you ask questions, I will record them on the whiteboard connected by arrows.
}

@teacher{

This activity focuses on showing students the process of building a *decision tree*. The diagram on the right models a game with the secret word "knife". Note that this diagram is *not* a complete decision tree, but a portion of one. The remainder of the tree will grow over the course of the lesson.

If you want your tree to look similar to the sample, choose "knife" as the first secret word, then help students to notice that there are three utensils on the list of items. That said, there is no need for your class diagram to be an exact copy of the sample provided.

@right{@image{images/tree1.png, 150}}

The first round of this game will produce a diagram similar to the one on the right. Your diagram will probably have two or three arrows, depending on the questions your students pose.

Important decision tree conventions:

- Be sure to label arrows with "yes" or "no".

- "Yes" arrows point to the left, and "no" arrows point to the right. (The chain of arrows in the diagram is pointing 45 degrees to the left because both questions were answered "yes".)

}

@QandA{
@Q{How many questions did we ask in order to determine the correct object?}
@Q{How did we decide which questions to ask?}
@Q{Which items from our original list (spoon, knife, cup, plate, fork, and mug) did our very first question eliminate from the list of possibilities?}
@A{Responses will vary. If students asked, "Is it a utensil?", then the eliminated items would include: plate, bowl, and mug.}
}

Let's play _another_ round of the game—but this time, you will try to guess one of the items that we eliminated by asking our original question. As you ask questions, I will add to our previous diagram.

@teacher{

@right{@image{images/tree2.png, 250}}

As students ask questions, add to your diagram.

Round 2 of this game will produce a diagram similar to the one on the right. Notice that the topmost question—"is it a utensil?"—now splits left ("yes, it is a utensil") *and* right ("no, it is not a utensil").

Be sure to always label arrows, as this is a standard convention and improves readability.

}

So far, our diagram offers two unique pathways from the top of the tree to two unique items.

Put another way: __There are two branches that split from the tree's root. Each branch leads to a decision node, which eventually leads to a terminal node (or "leaf").__

@lesson-instruction{
Let's identify the root, branches, nodes, and leaves on our tree so far.
}

@teacher{Discuss your in-progress tree to help students locate the different parts. The parts are intuitively named, but formal definitions are below if needed.}

@right{@image{images/terminology-tree.png, 175}}

- The root node is the very top node that represents the entire population or sample.
- Splitting is the process of dividing a node into sub-nodes with branches.
- Sub-nodes (or decision nodes) split from the root node, or from other nodes.
- A leaf node (or terminal node) is a node that does not split.

@vspace{1ex}

@lesson-instruction{
- With a partner, complete the decision tree on a piece of paper so that all six items are categorized.
- Now, draw an entirely different decision tree - one which has a __different__ question at its root.
}

@teacher{

@right{@image{images/tree3.png, 250}}

One possible completed tree is on the right.

To create a different tree, students can start with any of the questions from levels 2 or 3, or they can generate an entirely different starting question.

As students finish, invite them to draw their trees on the board. Ideally, you will have a wide assortment of trees! If students are not developing interesting trees, urge them to think of *entirely* different questions than those posed in the sample tree.

}

@QandA{
@Q{After looking at the decision trees of your classmates: What do these trees all have in common? How are they different?}
@A{Answers will vary. Each tree will have twice as many branches as nodes. Many trees will have the same number of nodes, although probably not all. Many questions asked will likely be the same, but not every question.}
@Q{Are all of the trees equally efficient?}
@A{The trees are probably similarly efficient, requiring either five or six nodes, including the root. Five nodes is more efficient than six.}
}

You might be wondering: Would it be possible to make this tree any more efficient? If so, why not?!

@lesson-instruction{
- With a partner, complete @printable-exercise{comparing-trees.adoc}.
}

As we increase a tree's efficiency, we can lose accuracy. Conversely, if we focus *too* much on accuracy - making our decision trees bigger and more complex - we risk __overfitting__. Overfitting happens when we teach a model the specific quirks of one particular dataset, preventing it from making reliable predictions about new data.

Decision trees can accurately label and categorize the inputs that they are trained to label and categorize! This tree—which was 100% accurate on the trained dataset—falters when we offer it inputs that are either *unknown*, like the chopstick, or *ambiguous*, like the spork. In both instances, the tree offers up its best guess at a label. __The only way this tree stands a chance of correctly identifying a chopstick or a spork is if we offer it more training!__

As we built our decision trees, we were able to draw on everything we know about every knife, spoon, spork, plate, bowl or mug that we have ever seen. If you were asked to create a decision tree to identify common animals or foods, you could probably do that without much difficulty as well.

Computers, however, build decision trees using only the data we provide... and that data can sometimes be messy. As a result, we may end up with models that are not 100% accurate.

@right{@image{images/iris-data.png, 300}}

Imagine that you are tasked with building a decision tree that can determine an iris' species (_setosa_, _virginica_, and _versicolor_) based on the varying plant measurements. It would be far more challenging to build this tree than the one we just made that classifies common tableware!

(British statistician and biologist Ronald Fisher first published his findings about the Iris Dataset in 1936. The dataset includes 150 different plants' sepal length, sepal width, petal length, and petal width. Today, this dataset is considered a go-to dataset useful for illustrating a wide range of problems in data science and machine learning.)

=== Synthesize

@QandA{

@Q{How is playing 20 questions to figure out a mystery item _similar_ to the predicting and recommending that a decision tree enables?}
@A{Decision trees, like 20 Questions participants, need to avoid questions that don't provide information. Decision trees, like 20 Questions participants, sometimes fail to identify an object correctly. Familiarity with the dataset improves the accuracy of both a decision tree and a 20 Questions participant.}

@Q{How is it different?}
@A{When we play 20 Questions, we are trying to identify a single object. Decision trees are useful for categorizing a variety of objects, or predicting outcomes in a variety of different scenarios.}
}



== Decision Trees from Training Datasets @duration{25 minutes}

=== Overview

Students build a decision tree that predicts whether different individuals will purchase a video game or not.

=== Launch

We have already built some extremely simple decision trees. We have a sense of the heirarchical structure, flexibility, and versatility of decision trees. We know that decision trees sometimes fail when provided with ambiguous or unknown data. We understand the risk of overfitting our data (building a model that can only succeed on narrow set of data).

We have *not* yet considered how useful, powerful decision trees built from large datasets can make relatively accurate predictions, recommendations, and diagnoses.

The key to building an effective decision tree is to decide—at every level and every node—which attributes are the most informative ones to ask questions about.

...but how do we make such decisions?!

It turns out, there's an algorithm for that, and it's relatively straightforward.


=== Investigate

Have you ever done some online shopping—say, for a new pair of sneakers—only to discover that, for the next several days, you encounter _advertisements for sneakers_ lurking in every corner of the internet that you visit?!

Is it a coincidence? No. Computer cookies are small data files stored locally on your device. One particular kind of cookie, the tracking cookie, allows AI designed for marketing to use your individual browsing habits to decide which ads you will be the most susceptible to.

We're going to create a decision tree that predicts whether or not different customers at a particular online store will purchase a video game or not. To do so, we must first train the computer! We will do so using a @handout{decision-tree-data.adoc, "training dataset"} that characterizes 14 different shoppers and then indicates whether each one purchased a video game or not.

@QandA{
@Q{What do you Notice about the @handout{decision-tree-data.adoc}? What do you wonder?}
@A{Possible responses: Individuals in their twenties always buy the video game. There are only three new customers; two out of three times, new customers buy the video game.}
@Q{Can you foresee any problems with making a decision tree based on this dataset? If so, what are they?}
@A{Responses will vary.}
}

One problem with this dataset is that _age is continuous_. That won’t work! We need to break these ages down into two different groups - which will become two different _branches_ that grow out of a _decision node_. *For now, let’s agree to create three groups: teenagers; twenties; and thirties.*

@lesson-instruction{
- We will complete @printable-exercise{level-1.adoc} together, starting with "age" as the root node.
- As we create the first level of our tree, you will discover a tool that we use to complete all the levels of our model: the decision stump.
}

As we move down the tree, our job is to figure out _which questions_ we should ask and _when_ we should ask them... just like when we play 20 Questions! Decision stumps will help us decide which questions produce a greater information gain.

@strategy{Why Start the Tree with "Age"?}{
Students will likely notice that we seemingly arbitrarily started the tree with "age" as the root node. _Extremely perceptive_ students may notice that for both "age" and "interest", the likelihood of a correct prediction is 10/14. In other words, starting with "interest in game" produces the same information gain as starting with "age" as the root... *so how do we decide?*

It turns out there is no *one* correct way to build a decision tree. In general, however, we want to avoid tall, skinny trees that pose one useless question after the other. Rather, it is beneficial to start with an attribute that will result in a _wider_ tree.

Because the "age" node splits _three_ ways and the "interest in game" node splits _two_ ways, we opt to start the tree with "age".
}

There are two possible questions we could use at the next level of our decision tree :

- Is the individual a frequent customer, an infrequent customer, or a new customer?
- Has the individual expressed interest in a particular video game?

@lesson-instruction{
- As you complete @printable-exercise{level-2.adoc} you will create and compare different __decision stumps__.
- These "stumps" will help you determine which question will produce the biggest information gain.
- Be ready to share which attributes you plan to add to the second level of your tree.
}

@lesson-instruction{
- Complete the first section @printable-exercise{build-and-test.adoc}, then let's share the rules we developed.
- "Test the Tree" by completing the second section of @printable-exercise{build-and-test.adoc}.
}

=== Synthesize

@QandA{
@Q{What are some reasons that a decision tree might produce an inaccurate prediction or recommendation?}
@A{If the tree has been designed to prioritize efficiency over accuracy, it may produce wrong predictions and recommendations. If the training dataset does not accurately represent the broader population, predictions and recommendations will be incorrect.}

@Q{After testing our tree, we discovered that it was not as accurate as we might have presumed. Can you think of any examples of when _missing data_ can create problems?}
@A{Responses will vary. When various populations are underrepresented in training datasets, the resulting technology reflects that, and we end up with AI that fails to meet the needs of those populations.}
}



