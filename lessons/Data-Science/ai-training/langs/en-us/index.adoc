= Training Artificial Intelligence

@description{Students }

@ifproglang{pyret}{
@lesson-prereqs{ds-intro}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...


| Student-facing Lesson Goals
|


| Materials
|[.materials-links]
@material-links

|===

== Build a Bag of Words

=== Overview

=== Launch

As a student, you probably know what it feels like to be under surveillance.

- When you enter your school, adults are stationed around the building and in the hallways. You might even go to a school that uses cameras to check that students are dressed and behaving a certain way.
- When you use the internet at your school or on a school-issued computer, software monitors your web use and blocks you from visiting a multitude of sites.
- When you submit an essay to your English teacher, you can expect that they will check for plagiarism - perhaps by running it through a plagiarism detector to be certain that all words and thoughts are your own.

Sure, these surveillance tactics have their purposes... but being constantly watched is exhausting! Let's turn the tables and see if we can _beat_ one method of surveillance: the plagiarism detector.

To ground our conversation, let's think about a very important question: Is it possible to _fool_ a plagiarism detector?

@QandA{
- Your teacher announces that they will be running all student writing through a plagiarism detector.
- You are a student who wants to plagiarize.
- Exercise some creativity: What are your strategies for evading detection?
}

@strategy{Adversarial Thinking}{
Go easy on your students! As students share their plagiarism strategies, you may feel judgmental. We urge you to keep those feelings at bay.

In this exercise, we are trying to get students to engage in *Adversarial Thinking* (put simply, thinking like a hacker). This is a valuable strategy that is taught, for example, in cybersecurity courses at the university level. Security, data protection, and even consideration of the harms caused by AI—these all require adversarial thinking skills. Adversarial Thinking is a valuable skill for students to develop; the key is that they learn how to exercise it in an ethical way!

Instead of concluding that students who excel at thinking in this way are ethically compromised, consider commending their creativity and reasoning.
}

To understand the workings of plagiarism detection, we'll start by looking at a simple detector that _does not work very well_. First, it consumes documents from the internet. Next, we feed it a student student-submitted document. It compares the student document against the others to determine if there is a match.

@lesson-instruction{
- Open the @starter-file{plagiarism}.
- With a partner, complete @printable-exercise{primitive-plagiarism-detector.adoc}.
}

If the plagiarism detector finds a match, we can be certain that an identical document exists. If the detector does not find a match, we know that there are no identical documents. _Either way, we can't draw any conclusions about whether plagiarism happened!_

As we discussed, plagiarizers usually alter at least a few words of the original document. Sometimes they change the ordering of the text, and sometimes they delete a sentence or word here and there. *We need a plagiarism detector with more sophistication!*

=== Investigate

Detecting identicality is not good enough. We need a different approach. We need to determine the _closeness_ of two documents. To do that, we need a way to summarize each document, and then compute the distance between the summaries.

One standard way to summarize a document is by creating a "bag of words" model. Let's look at two documents (below). Each is an example of jazz "scatting", when a vocalist improvises with nonsense syllables. Consider the two strings, below.

- Document a: "doo be doo be doo"
- Document b: "doo doo be doo be"

The bag-of-words summary for Document A looks like this: `"doo": 3, "be": 2`

As you can see, we've taken the original sentence and disregarded word order, creating a collection that focuses solely on *word frequency*.

With our bag of words, we have actually created a vector where each word represents one axis. Using vector notation, we can represent Document a like this: @math{\overrightarrow{a} = (3, 2)}

If we were to plot a point for the vector on the coordinate plane, it would produce this:

@center{@image{images/3-2.png, 150}}

@QandA{
@Q{What is the bag-of-words summary for Document b?}
@A{Some students may conclude that @math{\overrightarrow{a} = (3, 2)}, while others may argue that @math{\overrightarrow{a} = (2, 3)}. Order does not matter in a Bag of Words... but we do need to _choose_ an order that we will use for all of the vectors in the space.}
@Q{How would you represent the vector for Document b on the coordinate plane?}
@A{The point would be in the exact same position as the point for Document a. When we plot a point on the coordinate plane, first we plot @math{x} and then we plot @math{y}. There is no such protocol with the bag-of-words model. That said, it is crucial to adhere to the _same word order_ for each Bag of Words. Because we decided on "doo" then "be" for document a, we must use "doo" then "be" for document b also.}
}

@lesson-point{
A bag-of-words model represents text as an unordered collection of words with frequencies.
}


The bag-of-words summary for both documents is exactly the same! When the program takes stock of word frequency and ignores literally everything else, the two models are a perfect match: each one results in a point on the coordinate plane at @math{(3,2)}.


=== Synthesize

@QandA{

The bag-of-words model is better at detecting plagiarism than the primitive plagiarism detector - but it's not perfect.

@Q{What kind of plagiarism _can_ we catch using this model?}
@A{We can catch a plagiarizer who reorders the words a document.}

@Q{What sort of plagiarism are we still _unable_ to catch?}
@A{We cannot catch a plagiarizer who _alters_ the words in a document by substituting in synonyms or changing word tense.}

@Q{What might we _misidentify_ as plagiarism using this model? Put another way, what sort of _non-plagiarism_ might be labeled _plagiarism_?}
@A{Someone might independently write a text with a Bag of Words that happens to be quite close to the Bag of Words for a different text. This coincidence is more likely with shorter documents. Returning to our Documents a and b: scatting jazz vocalists are not commonly accused of stealing one another's material.}
}


== Normalize Data and Consider Dimensionality

=== Overview

=== Launch


Documents a and b were relatively simple. Because we used a total of two words, we needed only two axes to plot our vectors - the "be" axis and the "doo" axis.

Let's look at some slightly more complicated documents:

- Document c: "doo be doo be doo doo doo"

- Document d: "be bop bop bop be bop bop"


[cols="1,2,2", options="header", stripes="none"]
|===

| Document
| Bag-of-words summary
| Vector

| c
| `"doo": 5, "be": 2`
| @math{\overrightarrow{c} = (5, 2)}

| d
| `"bop": 5, "be": 2`
| @math{\overrightarrow{d} = (5, 2)}

|===

*We have a problem.*  We can plainly see that Documents c and d and are *not* the same ... but their vectors are. _What went wrong here?_

=== Investigate

To solve this problem, let's start by taking a closer look at our data.

First we must recognize that between Documents c and d there are *three* different words. Because there are three words, we need to use a *three* dimensional space, rather than a coordinate plane, which has just two dimensions. We can use a Venn Diagram to visualize our corpus:

@center{@image{images/scat-venn-diagram.png, 250}}

We must revise our bag-of-words summaries and our vectors!

@teacher{Including ZERO is necessary.... explain.}

The new bag-of-words summary for Document c is `"doo": 5, "be": 2, "bop": 0`, which we can represent as  @math{\overrightarrow{c} = (5, 2, 0)}.

The new bag-of-words summary for Document d is `"doo":0, "be": 2, "bop": 5`, and we can represent it as @math{\overrightarrow{d} = (0, 2, 5)}.

It is a bit trickier to envision plotting these vectors - but not impossible!

USE DIAGRAMS INSTEAD

For @math{\overrightarrow{c}}, envision a sheet of paper resting on a table. Plot @math{(5, 2)} on that sheet of paper: move 5 units to the right of the origin and then 2 units up. Because the z-coordinate is 0, the piece of paper *stays on the table.*

For @math{\overrightarrow{d}}, again envision a sheet of paper resting on a table. Plot @math{(0, 2)} on that sheet of paper by moving 2 units along the y-axis above the origin. Because the z-coordinate is 5, we imagine lifting the sheet of paper off the table and increasing its height (z) by 5-units.


=== Synthesize





== Compute Closeness and Exercise Human Judgment

=== Overview

=== Launch

The training phase is now complete. Let's review what has happened so far.

*1. We created bag-of-words models of our documents.*

In doing so, we compressed the data by isolating the single feature that we care about: word frequency. That made the data more useful to us. As a result, the _new_ representation of the data became considerably smaller than the actual corpus.

@lesson-point{
Loss of data is a common and often necessary effect of training AI!
}

When we discussed @lesson-link{ai-data-driven-algorithms}, we learned that we get a higher quality output when we provide the computer with _more_ data. It is no surprise, then, that _compressing data_ is often a critical element of training AI.

@strategy{How long does it take to train AI?}{
The plagiarism detector we will use, built in Pyret, trains on just a single text. Consequently, the training happens almost instantaneously.

Plagiarism detectors with bells and whistles, however, train on hundreds of thousands of texts collected from the internet. Like the training of ChatGPT (which took months!), this is a much more costly and time-intensive process.

AI really took off around 2010 because, at this time, more resources became available to train AI. Some, but not all, of these resources included: the increase of available data on the internet and the increased availability of graphics processing units (GPUs) to enable more efficient training.
}


*2. We normalized our data.*

Comparisons are most useful when we are comparing items that are alike. When building bags of words for the documents of corpus, each model *must* have the same number of words (dimensions!) regardless of how many words are in a given document. Defaulting to a cliche: we need an "apples-to-apples" comparison, rather than an "apples-to-oranges" comparison. This is why include in some models words that were not encountered in a given document.



=== Investigate

When we execute the program, the plagiarism detector computes the student's vector (the input!) and compares it against the other vectors. To do this, it uses the `cosine-similarity` function.

@strategy{That Cosine?!}{

You might be wondering: are we actually using *that* cosine—the one students learn about when studying trigonometry? The answer is YES!

The `cosine-similarity` function computes the cosine of the angle between two vectors. While it is not necessary for students to understand the mathematics happening behind the scenes, the function is a vital part of the program... and a lovely answer to the often-asked question, "Where are we ever going to use this?"

}


To allow for a pleasant user experience, a modern plagiarism detector does not actually provide a representation of a multi-dimensional space with varying points. That would be too complicated! Although different plagiarism detectors provide different outputs for their users, here's how the one in Pyret works.

- The `cosine-similarity` function takes in two strings (documents).
- The plagiarism detector produces an output of 1 when the vectors are identical.
- The plagiarism detector produces an output of zero when the vectors are entirely different.
- The plagiarism detector produces a value between zero and 1 for all other comparisons, reflecting the level of similarity of two bags of words.

@lesson-instruction{
- Complete @printable-exercise{human-judgment.adoc}.
}



=== Synthesize

