= Decision Trees

@description{Students play a game to explore how decision trees work, before building their own decision trees from training datasets.}

@ifproglang{pyret}{
@lesson-prereqs{ds-intro}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

- Create a decision tree from a training dataset.
- Explain contexts in which they might encounter decision tree AI.


| Student-facing Lesson Goals
|

Let's create, use, and understand decision trees!

| Materials
|[.materials-links]
@material-links


|===

== What is a Decision Tree? @duration{25 minutes}

=== Overview

Students are introduced to scenarios in which decision trees are used, and then play a game to help them build a frame of reference.

=== Launch

Consider each of three situations described below:

- You just finished watching a youtube video. Another video appears... and it is exactly the kind of video you would want to watch.
- Two individuals who work for the same company and earn the same salary apply for a loan by filling out the same online form at the local bank. One of them is granted a loan, the other is not.
- A news anchor displays a map with some states colored blue and others colored red. She explains that the map's predictions for the upcoming election were developed using historical census data.

On the surface, these situations may not seem to have a lot in common. If you dig a little deeper, however, you will discover that machine learning is at work in all three. In each of these scenarios, a computer uses data to drive the way that it makes recommendations, decisions, and predictions.

@QandA{
Think about those three scenarios: recommending videos, classifying loan requests, and predicting election outcomes. What sort of data do you think was used in each?
}

A @vocab{decision tree} is a type of machine learning that categorizes and makes predictions based on how a previous set of questions was answered. The scenarios above all use decision trees to arrive at their recommendations. Decision trees are a form of _supervised_ machine learning because the computer is trained on data that contains the desired categorizations.

=== Investigate

Decision trees pop up everywhere because they are flexible and easy to interpret—and they are flexible and easy to interpret because they function similarly to human thinking and decision-making.

As a class, we are going to play a game to help us understand the sort of thinking that decision trees imitate.

@teacher{The game we are going to play is a simplified version of 20 questions. If students are familiar with 20 questions, discussing the strategies they use when playing the game will help them to make connections.}
@QandA{
@Q{Show of hands: Have you ever played 20 Questions?}
@A{*If the majority of your students haven't played 20 Questions, the discussion questions below are irrelevant - skip straight to the instructions about how our game will work*}
@Q{What are the rules?}
@A{1 player is “it”. They have to think of a secret person, place or thing. The other players can ask up 20 yes or no questions to help them guess the secret person, place, or thing.}
@Q{What strategies do you use when choosing the questions you will ask?}
@A{Possible responses:}
@A{Ask questions that are likely to eliminate many objects... but be careful! A question that is too narrow could end up resulting in very little information gained.}
@A{Start with questions that will quickly limit possibilities.}
}

@lesson-instruction{

We're going to play a modified version of that game. Here's how it works:

- In our game there are six possible items: *spoon, knife, cup, plate, fork, and mug*.

- I will write one of them down on a piece of paper.

- You will ask me yes/no questions (however many you need!) until you figure out the object that I chose.

- As you ask questions, I will record them on the board, connecting the questions, answers, followup questions, and their answers with arrows.
}

@teacher{

@right{@image{images/spoon-game.png, 175}}

This activity focuses on showing students the process of building a *decision tree*. The first round of this game will produce a diagram similar to the one on the right, which models a game with the secret word "spoon".  The number of questions required to "guess" the object - most likely between two and four - will depend on what your students ask.

Note that this diagram is *not* a complete decision tree, but a portion of one. The remainder of the tree will grow over the course of the lesson.

Important decision tree conventions:

- Be sure to label arrows with "yes" or "no".

- "Yes" arrows point to the left, and "no" arrows point to the right. 

As students ask questions, support them in noticing how effective their questions are in sorting the items on the list. "Does it have a handle?" split the list between three options that didn't have handles and four that did. Other questions could have guaranteed eliminating three choices (half of them!) right off the bat.
}

@QandA{
@Q{How many questions did we ask in order to determine the correct object?}
@Q{How did we decide which questions to ask?}
@Q{Which items from our original list (spoon, knife, cup, plate, fork, and mug) did our very first question eliminate from the list of possibilities?}
@Q{Can anyone think of a different question that would have eliminated more items right off the bat?}
}

Let's play _another_ round of the game with a new item. 

@QandA{
@Q{How many questions did we ask in order to determine the correct object this time?}
@Q{How did we decide which questions to ask?}
@Q{Which items from our original list (spoon, knife, cup, plate, fork, and mug) did our very first question eliminate from the list of possibilities?}
@Q{How are the diagrams we drew similar and how are they different?}
}

Let's imagine that our first round had started with the question, "Is it a utensil?" and had led us to "knife".  After the first round, our tree might have looked the diagram on the left (below). If the second round started with the same question, we could have just added to the original diagram... and we might have ended up with something like what you see on the right.

[cols="^.2a,<.^1a,3a", grid="none", frame="none", stripes="none"]
|===

| @hspace{8em}**Round 1**

|

| @hspace{8em}**Round 2**

| @image{images/tree1.png, 200}

| @image{images/arrow.png, 50}

| @image{images/tree2.png, 370}

|===


Notice that the topmost question—"is it a utensil?"—now splits left ("yes, it is a utensil") *and* right ("no, it is not a utensil").

Our diagram begins with two unique pathways from the top of the tree to two unique items.

Put another way: __There are two branches that split from the tree's root node. Each branch leads to a decision node, which eventually leads to a leaf node.__

@lesson-instruction{
Let's identify the root node, branches, decision nodes, and leaf nodes on our tree so far.
}

@teacher{Discuss your in-progress tree to help students locate the different parts. The parts are intuitively named, but formal definitions are below if needed.}

@right{@image{images/terminology-tree.png, 300}}

- The root node is the very top node that represents the entire population or sample.
- Splitting is the process of dividing a node into sub-nodes with branches.
- Decision nodes split from the root node, or from other nodes.
- A leaf node is a node that does not split.

@vspace{1ex}

@lesson-instruction{
- With a partner, turn to @printable-exercise{decision-tree.adoc} and complete the decision tree so that all six items are categorized.
- Then, draw an entirely different decision tree - one which has a __different__ question at its root.
}

@teacher{

@right{@image{images/tree4.png, 400}}

One possible completed tree is on the right.

To create a different tree, students can start with any of the questions from levels 2 or 3, or they can generate an entirely different starting question.

As students finish, invite them to draw their trees on the board. Ideally, you will have a wide assortment of trees! If students are not developing interesting trees, urge them to think of *entirely* different questions than those posed in the sample tree.

}

@QandA{
@Q{After looking at the decision trees of your classmates: What do these trees all have in common? How are they different?}
@A{Answers will vary. Each tree will have twice as many branches as nodes. Many trees will have the same number of nodes, although probably not all. Many questions asked will likely be the same, but not every question.}
}

Let's take a step back and see how well some of our decision trees will perform.

@lesson-instruction{
Complete Q1 on @printable-exercise{comparing-trees.adoc}.
}

@teacher{
Invite students to share and explain their responses before emphasizing the main ideas, below.
}

You just observed how a decision tree can accurately label and categorize the inputs _that it has been trained to label and categorize_. However, the tree we tested—which was 100% accurate on the training dataset—faltered when we offered it inputs that were either *unknown*, like the chopstick, or *ambiguous*, like the spork.

The only way this tree stands a chance of correctly identifying a chopstick or a spork is if we offer it more training!


@lesson-instruction{
Complete the remainder of @printable-exercise{comparing-trees.adoc}.
}

@QandA{
@Q{Why is it advantageous for AI to be efficient?}
@A{Responses will vary, but may include: reduced delays, an improved user experience, greater scalability, decreased environmental impact.}
@Q{Can you think of any reasons *not* to maximize an AI's efficiency?}
@A{Responses will vary, but students will likely observe that an increase in efficiency leads to a decrease in accuracy.}
}

In AI, efficiency and accuracy are often in conflict:

- AI is *efficient* when the computer performs a task with minimal time, memory, energy or data.

- AI is *accurate* when the computer performs its task with correct, relevant, and consistent results.

Striking the perfect balance is an ongoing challenge for computer scientists, and it is a challenge with far-reaching implications.

=== Synthesize

As we built our decision trees, we were able to draw on everything we know about every knife, spoon, spork, plate, bowl or mug that we have ever seen. And our decision tree didn't know about the utensils we either didn't know about yet or forgot to include. If you were asked to create a decision tree to identify common animals or foods, you could probably do that without much difficulty as well. 

Computers build decision trees using only the data we provide... and that data can sometimes be limited or messy. As a result, we may end up with models that are not 100% accurate.

Imagine that you are tasked with building a decision tree that can determine an iris' species (_setosa_, _virginica_, and _versicolor_) based on the varying plant measurements. It would be far more challenging to build this tree than it was to build the common tableware classification tree we just made because... !

@centered-image{images/iris-data.png}



== Decision Trees from Training Datasets @duration{25 minutes}

=== Overview

Students build a decision tree that predicts whether different individuals will purchase a video game or not.

=== Launch

We have already built some extremely simple decision trees. We have a sense of the hierarchical structure, flexibility, and versatility of decision trees. We know that decision trees sometimes fail when provided with ambiguous or unknown data. We understand the risk of overfitting our data (building a model that can only succeed on a narrow set of data).

We have *not* yet learned about how decision trees built from large datasets make relatively accurate predictions, recommendations, and diagnoses.

The key to building an effective decision tree is to decide—at every level and every node—which attributes are the most informative ones to ask questions about.

...but how do we make such decisions?!

It turns out, there's an algorithm for that, and it's relatively straightforward.


=== Investigate

Have you ever done some online shopping—say, for a new pair of sneakers—only to discover that, for the next several days, you encounter _advertisements for sneakers_ lurking in every corner of the internet that you visit?!

Is it a coincidence? No. On a computer, 'cookies' are small data files that a website can store on your device. These can be used to remember where you were the last time you visited a site, or a setting that you changed and want to keep the next time you visit a site. One particular kind of cookie, the tracking cookie, allows AI designed for marketing to use your individual browsing habits to decide which ads you will be the most susceptible to.

We're going to create a decision tree that predicts whether or not different customers at a particular online store will purchase a video game or not. To do so, we must first train the computer! We will use a training dataset that characterizes 14 different shoppers and then indicates whether each one purchased a video game or not.

@QandA{
@Q{With your partner, look over the @handout{decision-tree-data.adoc, Training Dataset}. What do you Notice? What do you wonder?}
@A{Possible responses:}
@A{Individuals in their twenties always buy the video game.} 
@A{There are only three new customers; two out of three times, new customers buy the video game.}
@Q{Can you foresee any problems with making a decision tree based on this dataset? If so, what are they?}
@A{Responses will vary.}
}

One problem with this dataset is that _age is continuous_. That won’t work! We need to break these ages down into different groups - which will become different _branches_ that grow out of a _decision node_. *For now, let’s agree to create three groups: teenagers; twenties; and thirties.*

@lesson-instruction{
- We will complete @printable-exercise{level-1.adoc} together, starting with "age" as the root node.
- As we create the first level of our tree, you will discover a tool that we use to complete all the levels of our model: the decision stump.
}

As we move down the tree, our job is to figure out _which questions_ we should ask and _when_ we should ask them... just like when we play 20 Questions! Decision stumps will help us decide which questions produce a greater information gain.

@strategy{Why Start the Tree with "Age"?}{
Students will likely notice that we seemingly arbitrarily started the tree with "age" as the root node. _Extremely perceptive_ students may notice that for both "age" and "interest", the likelihood of a correct prediction is 10/14. In other words, starting with "interest in game" produces the same information gain as starting with "age" as the root... *so how do we decide?*

It turns out there's more than one correct way to build a decision tree! In general, however, we want to avoid tall, skinny trees that pose one useless question after the other. Rather, it is beneficial to start with an attribute that will result in a _wider_ tree.

Because the "age" node splits _three_ ways and the "interest in game" node splits _two_ ways, we opt to start the tree with "age".
}

There are two possible questions we could use at the next level of our decision tree :

- Is the individual a frequent customer, an infrequent customer, or a new customer?
- Has the individual expressed interest in a particular video game?

@lesson-instruction{
- As you complete @printable-exercise{level-2.adoc} you will create and compare different __decision stumps__.
- These "stumps" will help you determine which question will produce the biggest information gain.
- Be ready to share which attributes you plan to add to the second level of your tree.
}

@lesson-instruction{
- Complete the first section @printable-exercise{build-and-test.adoc}, then let's share the rules we developed.
- "Test the Tree" by completing the second section of @printable-exercise{build-and-test.adoc}.
}

=== Synthesize

@QandA{
@Q{What are some reasons that a decision tree might produce an inaccurate prediction or recommendation?}
@A{If the tree has been designed to prioritize efficiency over accuracy, it may produce wrong predictions and recommendations. If the training dataset does not accurately represent the broader population, predictions and recommendations will be incorrect.}

@Q{After testing our tree, we discovered that it was not as accurate as we might have presumed. Can you think of any examples of when _missing data_ can create problems?}
@A{Responses will vary. When various populations are underrepresented in training datasets, the resulting technology reflects that, and we end up with AI that fails to meet the needs of those populations.}
}



