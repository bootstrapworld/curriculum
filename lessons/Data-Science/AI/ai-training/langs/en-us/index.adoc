= Training Artificial Intelligence

@description{Students consider what AI training and sampling are by interacting with a plagiarism detector. As a result of this exploration, they learn that training AI is both a resource-intensive and time-intensive process.
}

@ifproglang{pyret}{
@lesson-prereqs{ai-data-driven-algorithms}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...


| Student-facing Lesson Goals
|


| Materials
|[.materials-links]
@material-links

|===

== Build a Bag of Words

=== Overview

Students engage in adversarial thinking to determine basic requirements of a successful plagiarism detection program.


=== Launch

As a student, you probably know what it feels like to be under surveillance.

- When you enter your school, adults are stationed around the building and in the hallways. You might even go to a school that uses cameras to check that students are dressed and behaving a certain way.
- When you use the internet at your school or on a school-issued computer, software monitors your web use and blocks you from visiting a multitude of sites.
- When you submit an essay to your English teacher, you can expect that they will check for plagiarism - perhaps by running it through a plagiarism detector to be certain that all words and thoughts are your own.

Sure, these surveillance tactics have their purposes... but being constantly watched is exhausting! Let's turn the tables and see if we can _beat_ one method of surveillance: the plagiarism detector.

To ground our conversation, we will consider a very important question: Is it possible to _fool_ a plagiarism detector?

@QandA{
- Your teacher announces that they will be running all student writing through a plagiarism detector.
- You are a student who wants to plagiarize.
- Exercise some creativity: What are your strategies for evading detection?
}

@strategy{Adversarial Thinking}{
Go easy on your students! As students share their plagiarism strategies, you may feel judgmental. We urge you to keep those feelings at bay.

In this exercise, we are trying to get students to engage in *Adversarial Thinking* (put simply, thinking like a hacker). This is a valuable strategy that is taught, for example, in cybersecurity courses at the university level. Security, data protection, and even consideration of the harms caused by AI—these all require adversarial thinking skills. Adversarial Thinking is a valuable skill for students to develop; the key is that they learn how to exercise it in an ethical way!

Instead of concluding that students who excel at thinking in this way are ethically compromised, consider commending their creativity and reasoning.
}

To understand the workings of plagiarism detection, we'll start by looking at a simple detector that _does not work very well_. First, it consumes documents from the internet. Next, we feed it a student student-submitted document. It compares the student document against the others to determine if there is a match.

@lesson-instruction{
- Open the @starter-file{plagiarism}.
- With a partner, complete @printable-exercise{primitive-plagiarism-detector.adoc}.
}

If the plagiarism detector finds a match, we can be certain that an identical document exists. If the detector does not find a match, we know that there are no identical documents. _Either way, we can't draw any conclusions about whether plagiarism happened!_

As we discussed, plagiarizers usually alter at least a few words of the original document. Sometimes they change the ordering of the text, and sometimes they delete a sentence or word here and there. *We need a plagiarism detector with more sophistication!*

=== Investigate

Detecting identicality is not good enough. We need a different approach. We need to determine the _closeness_ of two documents. To do that, we need a way to summarize each document, and then compute the distance between the summaries.

One standard way to summarize a document is by creating a "bag of words" model. Let's look at two documents (below). Each is an example of jazz "scatting", when a vocalist improvises with nonsense syllables.

- Document a: "doo be doo be doo"
- Document b: "doo doo be doo be"

The bag-of-words summary for Document A looks like this: `"doo": 3, "be": 2`

As you can see, we've taken the original sentence and disregarded word order, creating a collection that focuses solely on *word frequency*.

With our bag of words, we have actually created a @vocab{vector} where each word represents one axis.


@lesson-point{
A @vocab{vector} is an ordered list of numbers within parentheses and separated by commas, representing a point.
}

Using vector notation, we can represent Document a like this: @math{\overrightarrow{a} = (3, 2)}

@teacher{
Some students may conclude that @math{\overrightarrow{a} = (3, 2)}, while others may argue that @math{\overrightarrow{a} = (2, 3)}. Order does not matter in a Bag of Words... but we do need to _choose_ an order that we will use for all of the vectors in the space.
}

If we were to plot a point for the vector on the coordinate plane, it would produce this:

@center{@image{images/3-2.png, 150}}

@QandA{
@Q{What is the bag-of-words summary for Document b?}
@A{The bag-of-words summary for Document A looks like this: `"doo": 3, "be": 2`}

@Q{How would you represent the vector for Document b on the coordinate plane?}
@A{The point would be in the exact same position as the point for Document a. When we plot a point on the coordinate plane, first we plot @math{x} and then we plot @math{y}. There is no such protocol with the bag-of-words model. That said, it is crucial to adhere to the _same word order_ for each Bag of Words. Because we decided on "doo" then "be" for document a, we must use "doo" then "be" for document b also.}
}

@lesson-point{
A bag-of-words model represents text as an unordered collection of words with frequencies.
}

The bag-of-words summary for both documents is exactly the same! When the program takes stock of word frequency and ignores literally everything else, the two models are a perfect match: each one results in a point on the coordinate plane at @math{(3,2)}.

=== Synthesize

@QandA{

The bag-of-words model is better at detecting plagiarism than the primitive plagiarism detector—but it's not perfect.

@Q{What kind of plagiarism _can_ we catch using this model?}
@A{We can catch a plagiarizer who reorders the words a document.}

@Q{What sort of plagiarism are we still _unable_ to catch?}
@A{We cannot catch a plagiarizer who _alters_ the words in a document by substituting in synonyms or changing word tense.}

@Q{What might we _misidentify_ as plagiarism using this model? Put another way, what sort of _non-plagiarism_ might be labeled _plagiarism_?}
@A{Someone might independently write a text with a Bag of Words that happens to be quite close to the Bag of Words for a different text. This coincidence is more likely with shorter documents. Returning to our Documents a and b: scatting jazz vocalists are not commonly accused of stealing one another's material.}
}


== Normalize Data and Consider Dimensionality

=== Overview

Students explore the importance of normalizing data, removing unneeded characteristics and eliminating redundancy.

=== Launch


Documents a and b were relatively simple. Because we used a total of two words, we needed only two axes to plot our vectors—the "be" axis and the "doo" axis.

Let's look at some slightly more complicated documents:

- Document c: "doo be doo be doo doo doo"

- Document d: "be bop bop bop be bop bop"


[cols="1,2,2", options="header", stripes="none"]
|===

| Document
| Bag-of-words summary
| Vector

| c
| `"doo": 5, "be": 2`
| @math{\overrightarrow{c} = (5, 2)}

| d
| `"bop": 5, "be": 2`
| @math{\overrightarrow{d} = (5, 2)}

|===

*We have a problem.*  We can plainly see that Documents c and d are *not* the same ... but their vectors are. _What went wrong here?_

@teacher{
Something definitely went wrong! The table above demonstrates the student error of *forgetting to normalize data and consider dimensionality*. Students discover what these entail during the lesson.
}

=== Investigate

To solve this problem, let's start by taking a closer look at our data.

First we must recognize that between Documents c and d there are *three* different words. Because there are three words, we need to use a *three* dimensional space, rather than a coordinate plane, which has just two dimensions. We can use a Venn Diagram to visualize our corpus:

@center{@image{images/scat-venn-diagram.png, 150}}

We must revise our bag-of-words summaries and our vectors!

@teacher{Normalizing data and considering dimensionality requires that--when a word occurs zero times--we acknowledge it. Instead of glossing over the dimension, we indicate that a given word occurred zero times.}

The new bag-of-words summary for Document c is `"doo": 5, "be": 2, "bop": 0`, which we can represent as  @math{\overrightarrow{c} = (5, 2, 0)}.

The new bag-of-words summary for Document d is `"doo":0, "be": 2, "bop": 5`, and we can represent it as @math{\overrightarrow{d} = (0, 2, 5)}.

@right{@image{images/2pts.png, 200}}

It is a bit trickier to envision plotting these vectors, but not impossible!

For @math{\overrightarrow{c}}, envision a sheet of paper resting on a table. Plot @math{(5, 2)} on that sheet of paper: move 5 units to the right of the origin and then 2 units up. Because the z-coordinate is 0, the piece of paper *stays on the table.*

For @math{\overrightarrow{d}}, again envision a sheet of paper resting on a table. Plot @math{(0, 2)} on that sheet of paper by moving 2 units along the y-axis above the origin. Because the z-coordinate is 5, we imagine lifting the sheet of paper off the table and increasing its height (z) by 5-units.

@lesson-point{
Training is the act of transforming *data* into a *model*.
}

We started out with two documents.

Now that our training is complete, we have two points that exist at specific locations in a multi-dimensional space.

We are ready to put our model to use!

=== Synthesize






== Compute Closeness and Exercise Human Judgment

=== Overview



=== Launch

The training phase is now complete. Let's review what has happened so far.

*1. We created bag-of-words models of our documents.*

In doing so, we compressed the data by isolating the single feature that we care about: word frequency. As a result, the _new_ representation of the data became considerably smaller than the actual corpus.

@lesson-point{
Loss of data is a common and often necessary effect of training AI!
}

*2. We normalized our data.*

Comparisons are most useful when we are comparing items that are alike. When building bags of words for the documents in the corpus, each model *must* have the same number of words (dimensions!) regardless of how many words are in a given document. Defaulting to a cliche: we need an "apples-to-apples" comparison, rather than an "apples-to-oranges" comparison. This is why we include in some models words that we did not encounter in a given document.

What now?

=== Investigate

Our primitive plagiarism detector determined if two documents matched perfectly. That plagiarism detector was not especially useful.

A _more_ effective plagiarism detector will compute the student's vector (a point in a multi-dimensional space), and then compare it to the _other_ points in that space.

To do this, we can use the `cosine-similarity` function.

@strategy{That Cosine?!}{

You might be wondering: are we actually using *that* cosine—the one students learn about when studying trigonometry? The answer is YES!

The `cosine-similarity` function computes the cosine of the angle between two vectors. While it is not necessary for students to understand the mathematics happening behind the scenes, the function is a vital part of the program... and a lovely answer to the often-asked question, "Where are we ever going to use this?"
}

To allow for a pleasant user experience, a modern plagiarism detector does not actually provide a representation of a multi-dimensional space with varying points. That would be too complicated! Although different plagiarism detectors provide different outputs for their users, here's how the one in Pyret works.

- The `cosine-similarity` function takes in two strings (documents).
- The plagiarism detector produces an output of 1 when the vectors are identical.
- The plagiarism detector produces an output of zero when the vectors are entirely different.
- The plagiarism detector produces a value between zero and 1 for all other comparisons, reflecting the level of similarity of two bags of words.

@lesson-instruction{
- Complete the first section of @printable-exercise{human-judgment.adoc}, where you will evaluate the closeness of the student essay and the wikipedia article using the cosine-similarity function.
- Complete the remaining two sections of @printable-exercise{human-judgment.adoc}, where you will consider four possible outputs of a plagiarism detector that utilizes the cosine similarity function.
}



=== Synthesize

@QandA{

@Q{AI can be impressive... but human judgment is still critical. Why?}

@A{The cosine-similarity function produces a number - and that is all! It is still up to the teacher to decide how to make sense of that number. Over-reliance on programs can result in unfair outcomes.}

}
