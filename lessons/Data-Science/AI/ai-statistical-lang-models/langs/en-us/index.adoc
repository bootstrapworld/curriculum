= Statistical Language Modeling

@description{Students consider how statistics and probability drive AI text generation, learning that language models can produce text that sounds credible but may not actually be credible.}

@ifproglang{pyret}{
@lesson-prereqs{ai-training}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

@objectives

| Student-facing Lesson Goals
|

- Let's explore how probability influences the output of generative AI models.


| Materials
|[.materials-links]
@material-links

|===



== Introduction to Statistical Language Modeling

@objective{predictive-text}

=== Overview

Students explore how text prediction relies on statistical language modeling.

=== Launch

While texting, emailing, or even conducting a web search, you've probably noticed that your thoughts and sentences are sometimes finished for you! Predictive text is now a common feature of many apps, and it is made possible by a certain form of AI: the statistical language model.

@lesson-instruction{
- Consider this start of a phrase: "Close the..."
- On a piece of paper, write down the next word that pops into your mind.
- Write down two additional next words.
- As a class, we will generate a list of all of the words we suggested.
}

@teacher{
Students will think of words like "door", "box", "microwave", "laptop"... On the board, record their word suggestions. If a word is repeated, keep a tally of how many students suggest each particular word.
}

=== Investigate

Let's look a little more closely at the words we generated.

@QandA{
@Q{Which words had the _most_ tally marks?}
@Q{Which words had the _least_ tally marks?}
@Q{Are you surprised by the most and least common words? Why or why not?}
@Q{What was our class probability of proposing each of our top three words?}
@A{For example: If there are 25 students in your class, and 8 students chose “door”, the probability of a student choosing that word is 8/25.}
}

@teacher{If your students are comfortable computing the probability of choosing each word, they may do so in pairs or individually. If not, this can be a teacher-led task. Laying a solid foundation is worth the time: students will compute probability throughout the lesson.
}


@QandA{
@Q{Do you think a text messaging app with a predictive text feature would produce the same results as our class? Why or why not?}
@A{Invite conversation, and then share the image below to see the word options provided by one text messaging app.}
}

@right{@image{images/texting-app.png, 200}}


We've already discussed how AI systems are trained. Similarly, your predictive text feature is trained on a large amount of text.

During training, the computer extracts each use of the phrase "Close the" from the training data, along with the subsequent word. It then determines the probability of each possible next word. In this particular corpus, for this particular user, "door", "doors", and "house"  most commonly appeared after "Close the". As a result, those words are suggested for that user to choose from.

But are all users recommended the _same_ completions?

@QandA{
@Q{What do you think? Will your phone suggest the same words as your classmates' phones? Why or why not?}
@A{Refer to the in-depth student-facing explanation, below.}
}

@teacher{
Before revealing the answer, invite student dialogue. This question is a great opportunity to revisit previous concepts, in particular the process of training and sampling that students discussed when thinking about @lesson-link{ai-training, "plagiarism detection"}.
}

Not all phones will propose the same completions for a variety of reasons:

- There might be multiple words with equal probability, in which case the system must use some method of random selection.

- Systems are generally built with at least a little bit of randomness; the amount of randomness varies from system to system.

- A messaging app also includes a model of the text that _the actual user_ composes. The app therefore offers a degree of personalization based on what the user has chosen in the past. Similarly, the app can take into account the word suggestions that you have previously accepted.

- Some predictive text features even take into account the specific message that you are composing! For instance, typing first letter of an unusual word that you used in the same message triggers the app to propose that unusual word.

@QandA{
@Q{We just considered four reasons why different phones sometimes propose different completions. Do each of these four reasons represent _statistical_ phenomena? Why or why not?}
@A{Phenomena 1 and 2 are statistical, given that statistical language modeling always includes some element of randomness.}
@A{Phenomena 3 demonstrates the use of a _personalized_ language model, a more refined version of a statistical language model.}
@A{Phenomena 4 is almost anti-statistical! The AI consumes and uses data outside of the model for the corpus.}
}

Phenomena 3 and 4 above suggest that sometimes making a usable tool requires that we step out of bounds! Although pure statistical language models powerful, the upgrades that programmers develop can make the AI _better_ at completing the task that it was designed to complete.

You have just considered the workings and in-context use of a @vocab{statistical language model}. Hopefully you have discovered that, although it sometimes may _seem_ like your texting app can read your mind... it can't. It doesn't know the rules of grammar, the meanings of words, or your intentions when you are composing a text. It just knows *probability*, which it uses in ways that are often very impressive (but sometimes not!).

@teacher{
Throughout the lesson, we'll explore the very important "sometimes not" parenthetical, above.}

=== Synthesize

@QandA{

@Q{Can you imagine doing statistical language modeling for other spoken human languages? Which languages?}
@A{Statistical language modeling will work for any language. The AI does not need to "know" anything about the rules of grammar; it just follows rules that enable it to identify patterns.}

@Q{Are there things that are _not_ human spoken languages that a similar approach might work on?}
@A{Yes! With statistical language modeling, AI can compose music, play chess games, and more. The "text" need not be made up of words: any symbolic notation at all will do.}
}



== Constructing a Statistical Language Model

@objective{slm}

=== Overview

Students construct a statistical language model by decomposing the text and computing the probabilities of different words appearing.

=== Launch

The best way to make sense of statistical language modeling is to try it yourself! We'll start by constructing a model.

For our corpus, we will use the folk song @handout{old-lady-lyrics.adoc, "There Was an Old Lady Who Swallowed a Fly"}, which tells the nonsensical story of an old lady who swallows a fly, and the unfortunate series of events that follows.

First, we will decompose the title of our corpus into differently sized chunks (one word at a time, two words at a time, etc.):

[cols="^.^1,^.^1,<.^8", stripes="none", options="header"]
|===

| chunk size | Quantity			| Decomposition

| 1 word
| 9
| (There) (Was) (an) (Old) (Lady) (Who) (Swallowed) (a) (Fly)

| 2 words
| 8
| (There Was) (Was an) (an Old) (Old Lady) (Lady Who) (Who Swallowed) (Swallowed a) (a Fly)

| 3 words
| 7
| (There Was an) (Was an Old) (an Old Lady) (Old Lady Who) (Lady Who Swallowed) (Who Swallowed a) (Swallowed a Fly)

|===

The formal word computer scientists use in this context is not "chunk" but @vocab{n-gram}. In an @math{n}-gram, @math{n} represents the number of words in the chunk. For special cases where @math{n} is 1, 2, or 3, the @math{n}-grams are called unigrams, bigrams, and trigrams.


=== Investigate

Let's dig a little deeper...

@teacher{
To share the song lyrics with students,  have students @handout{old-lady-lyrics.adoc, "read them independently"}. If desired, you could also listen to a recorded version of the song.
}

The phrase "there was an old lady who swallowed a..." is repeated in our corpus! Let's zoom in on one unigram from that phrase: “there”.

@QandA{
@Q{Referring to the @handout{old-lady-lyrics.adoc, "lyrics"}: how many times does the word "there" appear in the song?}
@A{4}
@Q{In this corpus, how many times was the word "there" followed by the word "was"?}
@A{4}
@Q{What is the probability that the word "there" is followed by the word "was"?}
@A{4/4 or 100%}
}

In the example you just worked through, you computed the probability that "was" appears after the unigram "there". We can represent the computation you just completed with a special notation:

@math{p(was | there) =}
@math{\frac
	{\textit{count(there was)}}
	{\textit{count(there...)}}
= {\frac{4}{4}}}

Put another way: To compute the probability that "was" follows "there", we divide 4 (how many times we see "was" follow "there") by 4 (how many times we see "there" followed by anything).


@lesson-instruction{
- Complete @printable-exercise{constructing-model.adoc}.
}

@teacher{
Are you and your students interested in exploring probability in more depth? Check out @lesson-link{probability-inference} to dig deeper.
}


=== Synthesize

@QandA{
In our corpus, there were _three_ possible completions for the 3-gram "to catch the." There were _four_ possible completions for the unigram "the."

Based on these observations, we can conclude that _in this corpus_, as the value of @math{n}_increases_, the number of completion options _decreases_.

@Q{Do you think the above statement is true of other corpuses?}
@A{Yes, in general, this is a true statement: longer phrases have fewer possible completions than single words.}

}





== Sampling from the Model

=== Launch

Having built a language model, what can we do with it? We can use it in a generative way: we can produce output!

How might we go about doing that?

- We can start by choosing our first word. A common approach is to ask, "What's the most common @math{n}-gram in the corpus?" but we can also choose the starting word on our own, if we want.

- Next, we ask: "Given the first @math{n}-gram, what is the most common successor?"

- We repeat this second step forever! ...or, more realistically, until we decide to stop the program. A simple statistical language model, however, will generate text ad infinitum.

=== Investigate

Let's give this process a try, returning to our "Old Lady" corpus.

@lesson-instruction{

- Using @handout{old-lady-lyrics.adoc}, complete the first section of @printable-exercise{sampling.adoc}.
- Tip: You will be able to work more efficiently if you open the PDF of the handout on a computer and use "Control-F" on a PC or "Command-F" on a Mac to help you locate and count words.
}

@teacher{The two questions below are on students' worksheets, but merit follow-up and discussion.}

@QandA{
@Q{What four-word phrase did you generate?}
@A{"She swallowed a fly"}

@Q{Did everyone in your class end up with same phrase? How and why did that happen?}
@A{Yes. When considering which word to generate next, there was always one word that was clearly the most probable, an no ties.}

}

@lesson-instruction{
- Complete the second section of @printable-exercise{sampling.adoc}.
}

@QandA{
@Q{What four-word phrase did you generate?}
@A{The class should be split between "The spider to catch" and "The spider that wriggled".}


@Q{Did everyone in your class end up with same phrase? How and why did that happen?}

@A{No. Students arrived at different four-word phrases because we were forced to incorporate randomness when there was a tie for the most probable word to follow "spider".}
}

Modern statistical language models often invite users to adjust the @vocab{temperature} of the generated text, which influences the level of randomness. For instance, ChatGPT users are encouraged to use a _low_ temperature for more focused and less creative tasks. They are encouraged to use a _higher_ temperature for more random and increasingly creative tasks.

@lesson-point{
Temperature is the parameter that controls the randomness of the model's output as it generates text.
}

Even _without_ the ability to raise the temperature, we encountered randomness and variability in our generated texts. With a large enough corpus and a high enough temperature, a statistical language model will produce a new and unique output every single time!


@strategy{AI "Hallucinations"}{

As generative AI produces text, it often generates incorrect or misleading information. This is commonly known as an AI "hallucination".

Some experts dislike this term and are encouraging an end to its use. These experts argue that _all_ output is "hallucinatory". Some of it happens to match reality... and some does not.

The very same process that generates "hallucinatory" text _also_ generates the "non-hallucinatory" text. This truth helps us to understand _why_ it is so difficult to *fix* the "hallucination" problem.

This term also attributes intent and consciousness to the AI, giving it human qualities when it is merely executing a program exactly as it is intended to do.
}



=== Synthesize

@QandA{

Critics of ChatGPT and other language models raise a variety of concerns. Consider each of them, below.

@Q{This form of AI often has trouble with _facts_. Put another way, ChatGPT sometimes "makes stuff up." Why does this happen? What is actually going on?}

@A{When ChatGPT produces false or misleading information, it is not glitching nor is there a bug. ChatGPT is just doing what it does, following the model as it ought to.}


@Q{Others complain that ChatGPT has biases that can be seen in its text output. Where do these biases come from?}

@A{If there are biases in the corpus, there will likely be biases in the output.}
}





== What is the Role of Language?

=== Overview

Students consider whether statistical language modeling requires language.

=== Launch


@lesson-instruction{
- Play a game of tic-tac-toe with your neighbor.
- Play the tic-tac-toe game spelled out (Document A, on the right) with your neighbor.
@right{@image{images/docA.png, 100}}

- Not sure how to proceed?
** The tic-tac-toe board is a 3x3 coordinate plane.
** Each row of the document contains two pieces of information: (1) the player (X or O), and (2) the ordered pair for the selected cell.
}


=== Investigate

You've used a paper, pencil, and probability to apply the principles of statistical language modeling. It's time to peek behind the curtain and see how a computer can put this model to use! To make that happen, we're going to explore Soekia, a simplified text generation tool designed for student learning. As we explore, we are going to consider: What is the role of _language_ in a statistical language model?

@lesson-instruction{
- Go to @link{http://bootstrapworld.org/SoekiaGPT/}
- Click the "LOOK INSIDE" button at the top right of your screen.
- Scroll to the right until the green, right-most panel ("Documents") is in view.
- Click the "Collections" icon in the upper right corner.
- From the drop-down menu that appears, select "tic-tac-toe".
}



=== Synthesize



== Deep Dive: Soekia

=== Overview

=== Launch

Let's dig deeper into Soekia!

@lesson-instruction{
- Go to @link{http://Soekia.ch/GPT/?lang=en}
- Complete the first section of @printable-exercise{soekia-intro.adoc}.
- When you're done, let's do a quick survey: Raise your hand if your story was largely inspired by "Felicia and the Pot of Pinks".
}

@teacher{
The vast majority of students will have a story that is primarily sourced from "Felicia and the Pot of Pinks". On the next section of the worksheet, students will discover exactly _why_ this is the case. Feel free to use this mystery as incentive to move on to the next section of the page!
}

@lesson-instruction{
- Complete the second section of @printable-exercise{soekia-intro.adoc}.
}

@QandA{
@Q{Why were so many of our initial stories all about Felicia and the Pot of Pinks?}
@A{The green bar indicates how closely the document matches the box on the "Generate Text" panel. The story "Felicia and the Pot of Pinks" includes the word "tale" once, "fairy" four times, and the word "me" more than a dozen times. With these frequencies, it is a much closer match to "Write me a fairy tale" than any of the other documents.}
}

Let's review what we have done so far:

- We have interacted Soekia's text generation panel. With modern AI, the text generating interface is the only element that we are privy to. Unlike the AI we use daily, Soekia allowed us to glimpse which words and phrases came from which sources.

- We have also peeked at Soekia's documents panel, or corpus. This is a critical feature of all text-generating AI, but ordinarily, it is hidden from us. Soekia also revealed to us the level of alignment between each document and what _we_ typed into the box on the "Generate Text" panel.

Let's explore the two remaining panels!

@lesson-instruction{
- Turn to @printable-exercise{soekia-closer-look.adoc}.
- Be prepared to share your responses with the class.
}

@ifnotslide{
@teacher{
As students are working, you can share the three tips, below.
}
}

@ifslide{
Advance to the next slide for student-facing tips on navigating Soekia.
}


@slidebreak

If you feel overwhelmed as you work, here are some tips:

- Click "Pause" to review each of the four panels. Ask yourself, "How is _this_ panel related to each of the other panels, in particular, the _adjacent_ panels?"

- Get curious! *Clicking* is powerful. Each time you click, you access previously hidden information. You can click a document, an @math{n}-gram, a suggested word, or even words that appear on the text generation panel.

- To slow down text generation and to allow time to observe changes as they occur, click the "Choose yourself" icon and use your mouse to select words. (You will be prompted to do this in the next activity.)

@teacher{
After they complete the "Closer Look" worksheet, invite students to share out on what they learned. In particular, have students share their predictions and whether they were correct or not. See if, as a class, you can develop an understanding of any unexpected outcomes.
}

=== Investigate

Modern statistical language models often invite users to adjust the "temperature" of the generated text. For instance, ChatGPT users are encouraged to use a _low_ temperature for tasks that are more focused and less creative tasks. They are encouraged to use a _higher_ temperature for more random and increasingly creative tasks. But why? What does "temperature" actually represent?

@lesson-instruction{
- Turn to @printable-exercise{soekia-temperature.adoc}.
- Pause for class discussion once you have completed the first section.
}

As you discovered, @vocab{temperature} is the parameter that controls the randomness of the model's output as it generates text.

@QandA{
@Q{AI sometimes generates false or misleading information. Do you think this is more likely to occur at a high temperature or a low temperature? Explain.}
}





=== Synthesize

- A student argues that AI is a reliably correct and credible source of information. How would you respond?
