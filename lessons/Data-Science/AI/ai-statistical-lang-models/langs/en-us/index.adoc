= Statistical Language Modeling

@description{Students }

@ifproglang{pyret}{
@lesson-prereqs{ds-intro}
}

@ifproglang{codap}{
@lesson-prereqs{ds-intro}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

- Define and apply statistical language modeling.
- Explain how predictive text AI relies on probability.

| Student-facing Lesson Goals
|

- Let's explore how probability influences the output of generative AI models.

| Materials
|[.materials-links]
@material-links

|===



== Statistical Language Models Overview

=== Overview

Students explore how text prediction relies on statistical language modeling.

=== Launch

While texting, emailing, or even conducting a web search, you've probably noticed that your thoughts and sentences are sometimes finished for you! Predictive text is now a common feature of many apps, and it is made possible by a certain form of AI: the statistical language model.

@lesson-instruction{
- Consider this start of a phrase: "She is on the..."
- On a piece of paper, write down the next word that pops into your mind.
- Write down two additional next-word choices.
- As a class, let's generate a list of all of the words we developed.
}

@teacher{
Students will think of words like "phone", "way", "porch", and "fence"... @hspace{1em} Record their first choice responses on the board. If a word is repeated, keep a tally of how many students suggest each particular word. Highlight the top three choices for the class. Then, as a class, compute the probability of choosing each choice.
}

@QandA{
@Q{What is the probability of choosing each of our generated words?}
@A{For example: if there are 25 students in your class, and 8 students chose “phone”, the probability of a student choosing that word is 8/25.}

@Q{What word choices were the most probable? How about the least probable?}
@A{Responses will vary depending on the words your students offered.}

@Q{Do you think a text messaging app with a predictive text feature would produce the same results as our class?}
@A{Share the image below to see the word options provided by one text messaging app.}
}

@right{@image{images/texting-app.png, 200}}


We've already discussed how AI systems are trained. Similarly, your predictive text feature is trained on a large amount of text.

The image of one particular texting app proposing "porch", "way", and "phone" raises an interesting question: Will all users be recommended the _same_ completion?

The answer is *no*, for a few reasons:

- There might be multiple words with equal probability, in which case, the system must use some method of random selection.

- Systems are generally built with at least a little bit of randomness; the amount of randomness varies from system to system.

- A messaging app also includes a model of the text that _the actual user_ composes. The app can therefore offer a degree of personalization based on what the user has chosen in the past.

- Some predictive text features even take into account the specific message that you are composing! For instance, typing first letter of an unusual word that you used in the same message triggers the app to propose that unusual word.

During training, the computer extracts each use of the phrase "She is on the" from the training data, along with the subsequent word. It then determines the probability of each possible next word. In this particular corpus, for this particular user, "phone", "porch", and "way"  most commonly appeared after "she is on the". As a result, those words are suggested for that user to choose from.

=== Investigate

Here's another worthwhile question: *Will your texting app would propose the same completions for "She is on the " as it would for "The "?*


AI as tic tac toe opponent


=== Synthesize









== Constructing a Statistical Language Model

=== Launch

The best way to make sense of statistical language modeling is to try it yourself! We'll start by constructing a model.

For our corpus, we will use the folk song @handout{old-lady-lyrics.adoc, "There Was an Old Lady Who Swallowed a Fly"}, which tells the nonsensical story of an old lady who swallows a fly, and the unfortunate series of events that follows.

First, we will decompose the title of our corpus into differently sized chunks (one word at a time, two words at a time, etc.):

[cols="^.^1,^.^1,<.^8", stripes="none", options="header"]
|===

| chunk size | Quantity			| Decomposition

| 1 word
| 9
| (There) (Was) (an) (Old) (Lady) (Who) (Swallowed) (a) (Fly)

| 2 words
| 8
| (There Was) (Was an) (an Old) (Old Lady) (Lady Who) (Who Swallowed) (Swallowed a) (a Fly)

| 3 words
| 7
| (There Was an) (Was an Old) (an Old Lady) (Old Lady Who) (Lady Who Swallowed) (Who Swallowed a) (Swallowed a Fly)

|===

During statistical language modeling, the computer breaks a text into chunks and then assesses how likely it is that any _single_ word in the text will _follow_ that chunk. We call these chunks @vocab{n-grams}, where @math{n} represents the number of words in the chunk.


[cols="^.^1,^.^1,<.^8", stripes="none", options="header"]
|===

| n-gram | Quantity			| Decomposition

| 1-gram
| 9
| (There) (Was) (an) (Old) (Lady) (Who) (Swallowed) (a) (Fly)

| 2-gram
| 8
| (There Was) (Was an) (an Old) (Old Lady) (Lady Who) (Who Swallowed) (Swallowed a) (a Fly)

| 3-gram
| 7
| (There Was an) (Was an Old) (an Old Lady) (Old Lady Who) (Lady Who Swallowed) (Who Swallowed a) (Swallowed a Fly)

|===


=== Investigate

Let's dig a little deeper...

@teacher{
To share the song lyrics with students,  have students @handout{old-lady-lyrics.adoc, "read them independently"}. If desired, you could also listen to a recorded version of the song.
}


The phrase "there was an old lady who swallowed a..." is repeated in our corpus! Let's zoom in on one unigram from that phrase: “there”.

@QandA{
@Q{Referring to the @handout{old-lady-lyrics.adoc, "lyrics"}: how many times does the word "there" appear in the song?}
@A{4}
@Q{In this corpus, how many times was the word "there" followed by the word "was"?}
@A{4}
@Q{what is the probability that the word "there" is followed by the word "was"?}
@A{4/4 or 100%}
}


In the example you just worked through, you computed the probability that "was" appears after the unigram "there". We can represent the computation you just completed with a special notation:

@math{p(was | there) =}
@math{\frac
	{\text{count(there was)}}
	{\text{count(there...)}}
= {\frac{4}{4}}}

Put another way: To compute the probability that "was" follows "there", we divide 4 (how many times we see "was" follow "there") by 4 (how many times we see "there" followed by anything).


@lesson-instruction{
- Complete @printable-exercise{stat-lang-model-intro.adoc}.
}


=== Synthesize







== Sampling from the Model

=== Launch

Having built a language model, what can we do with it? We can use it in a generative way: we can produce output!

How might we go about doing that?

- We can start by asking, "What's the most common n-gram in the corpus?" _That n-gram becomes our initial output._

- Next, we ask: "Given _that_ n-gram, what is the most common successor?"

Let's give this process a try, returning to our "Old Lady" corpus.

=== Investigate


@lesson-instruction{

Complete @printable-exercise{sampling.adoc}.

}





@teacher{
Are you and your students interested in exploring probability in more depth? Check out @lesson-link{probability-inference} to dig deeper.
}

=== Synthesize

- Which will produce more grammatically correct responses: a 5-gram model or a 2-gram model? Explain.
- Which will produce a more creative response: a 5-gram model or a 2-gram model? Explain.




== Deep Dive: Soekia

=== Overview

=== Launch

You've used a paper, pencil, and probability to apply the principles of statistical language modeling. It's time to peek behind the curtain and see how a computer can put this model to use! To make that happen, we're going to explore Soekia, a simplified text generation tool designed for student learning.

@lesson-instruction{
- Go to @link{http://Soekia.ch/GPT/?lang=en}
- Complete the first section of @printable-exercise{soekia-intro.adoc}.
- When you're done, let's do a quick survey: Raise your hand if your story was largely inspired by "Felicia and the Pot of Pinks".
}

@teacher{
The vast majority of students will have a story that is primarily sourced from "Felicia and the Pot of Pinks". On the next section of the worksheet, students will discover exactly _why_ this is the case. Feel free to use this mystery as incentive to move on to the next section of the page!
}

@lesson-instruction{
- Complete the second section of @printable-exercise{soekia-intro.adoc}.
}

@QandA{
@Q{Why were so many of our initial stories all about Felicia and the Pot of Pinks?}
@A{The green bar indicates how closely the document matches the box on the "Generate Text" panel. The story "Felicia and the Pot of Pinks" includes the word "tale" once, "fairy" four times, and the word "me" more than a dozen times. With these frequencies, it is a much closer match to "Write me a fairy tale" than any of the other documents.}
}

Let's review what we have done so far:

- We have interacted Soekia's text generation panel. With modern AI, the text generating interface is the only element that we are privy to. Unlike the AI we use daily, Soekia allowed us to glimpse which words and phrases came from which sources.

- We have also peeked at Soekia's documents panel, or corpus. This is a critical feature of all text-generating AI, but ordinarily, it is hidden from us. Soekia also revealed to us the level of alignment between each document and what _we_ typed into the box on the "Generate Text" panel.

Let's explore the two remaining panels!

@lesson-instruction{
- Turn to @printable-exercise{soekia-closer-look.adoc}.
- Be prepared to share your responses with the class.
}

@ifnotslide{
@teacher{
As students are working, you can share the three tips, below.
}
}

@ifslide{
Advance to the next slide for student-facing tips on navigating Soekia.
}


@slidebreak

If you feel overwhelmed as you work, here are some tips:

- Click "Pause" to review each of the four panels. Ask yourself, "How is _this_ panel related to each of the other panels, in particular, the _adjacent_ panels?"

- Get curious! *Clicking* is powerful. Each time you click, you access previously hidden information. You can click a document, an N-gram, a suggested word, or even words that appear on the text generation panel.

- To slow down text generation and to allow time to observe changes as they occur, click the "Choose yourself" icon and use your mouse to select words. (You will be prompted to do this in the next activity.)

@teacher{
After they complete the "Closer Look" worksheet, invite students to share out on what they learned. In particular, have students share their predictions and whether they were correct or not. See if, as a class, you can develop an understanding of any unexpected outcomes.
}

=== Investigate

Modern statistical language models like ChatGPT often invite users to adjust the "temperature" of the generated text. For instance, ChatGPT users are encouraged to use a _low_ temperature for tasks that are more focused and less creative tasks. They are encouraged to use a _higher_ temperature for more random and increasingly creative tasks. But why? What does "temperature" actually represent?

@lesson-instruction{
- Turn to @printable-exercise{soekia-temperature.adoc}.
- Pause for class discussion once you have completed the first section.
}

As you discovered, @vocab{temperature} is the parameter that controls the randomness of the model's output as it generates text.

@QandA{
@Q{AI sometimes generates false or misleading information. Do you think this is more likely to occur at a high temperature or a low temperature? Explain.}
}


@strategy{AI "Hallucinations"}{

As generative AI generates text, it often generates incorrect or misleading information.

Some experts dislike this term, and are encouraging an end to its use. These experts argue that _all_ output is "hallucinatory". Some of it happens to match reality... and some does not.

The very same process generated the "hallucinatory" and "non-hallucinatory" text. This truth helps us to understand _why_ it is so difficult to *fix* the "hallucination" problem.

This term also attributes intent and consciousness to the AI, giving it human qualities when it is merely executing a program exactly as it is intended to do.

As students have discovered through their interaction with Soekia, all generated output - each and every word, sentence, and paragraph - is nothing more than a hallucination!
}



=== Synthesize

- A student argues that AI is a reliably correct and credible source of information. How would you respond?
