---
{layout="DS Title Slide"}
# Ethics, Privacy & Bias

<!--
To learn more about how to use PearDeck, and how to view the embedded links on these slides without going into present mode visit https://help.peardeck.com/en
-->

---
{layout="LaunchR"}
# Case Studies

@right{@image{images/AtomicBomb.png, 250}}

**Should scientists be bound by any ethical rules? If so, what rules?**{style="font-size:24pt"}


---
{layout="LaunchR"}
# Case Studies

@right{@image{images/AtomicBomb.png, 250}}

During World War II, scientists were engaged in a race to develop new weapons, more powerful than anything the world had ever seen. 

While the immediate goal was "win the war", many of the scientists realized that the weapons they were developing could be used for all sorts of things _after the war was over_ - **and not all of them were good.**

<!--

-->
---
{layout="Launch"}
# Case Studies

**Should doctors be bound by any ethical rules? If so, what rules?**{style="font-size:24pt"}

---
{layout="Launch"}
# Case Studies

Between 1932 and 1972, doctors in the US @link{https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study, gave syphilis to unknowing African-American citizens} as part of an experiment. In 1971, researchers at Stanford conducted an @link{https://en.wikipedia.org/wiki/Stanford_prison_experiment, experiment in human behavior that went horribly wrong, causing tremendous damage to their participants}.

---
{layout="Launch"}
# Case Studies

History is filled with examples of those who study science - be it medicine, human behavior, psychology, etc. - ignoring their ethical responsibility as humans.

With tech companies hiring Data Scientists at a staggering rate and collecting massive datasets on users for those scientists to mine, there's **a new arms race** happening right now.

---
{layout="Launch"}
# Case Studies

Search engines tailor their results based on what they know about the customer doing the search, and social media networks want to recommend friends based on what they know about all of us.

Both goals require building profiles on everyone, figuring out what their preferences are and where they tend to spend their time. They might require figuring out whether each of us is male or female, more likely to go to a movie or a play, or about to buy a dishwasher or a television.

<!--

-->

---
{layout="Launch"}
# Case Studies

But these datasets and profiles could be used for far more than that. **What if the FBI used them to try and figure out who is likely to commit a crime, or a company tries to learn their employees' religion or sexual orientation?**

---
{layout="Launch"}
# Case Studies

As they build ever-more sophisticated models based on ever-more accurate datasets, **Data Scientists need to think about the ethics of what they're doing** as well!{style="font-size:24pt"}

<!--

-->
---
{layout="Investigate"}
# Case Studies

Read one of the articles below, and complete @printable-exercise{pages/CaseStudy.adoc}.

- @online-exercise{https://www.forbes.com/sites/kashmirhill/2012/02/16/how-target-figured-out-a-teen-girl-was-pregnant-before-her-father-did/#68ca57a66686, How Target Figured Out A Teen Girl Was Pregnant Before Her Father Did}
- @online-exercise{https://www.cnn.com/2013/03/11/tech/social-media/facebook-likes-study/index.html, Facebook 'likes' can Reveal Your Secrets}
- @online-exercise{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing, Algorithmic Bias in Criminal Sentencing}

<!--
Divide the class into groups of 3-4, and assign each group a different case study. After they complete the worksheet, have each group choose one person to share back with the class.
-->
---
{layout="Synthesize"}
# Case Studies

- What are some commonalities and differences among the issues raised by these articles?
- **Should data scientists be bound by any ethical rules? If so, what rules?**

<!--
Give students time to discuss and share back. Encourage students to share back differing views on the articles.

-->
---
{layout="Supplemental"}
# Additional Exercises:
- Read this article on @opt-online-exercise{https://www.statnews.com/2020/06/17/racial-bias-skews-algorithms-widely-used-to-guide-patient-care/, Racial Bias in Algorithms used to determine Medical Care}. Write arguments for or against health care providers using algorithms to make medical decisions.
- Watch or read CBS News Coverage from when @opt-online-exercise{https://www.cbsnews.com/news/microsoft-shuts-down-ai-chatbot-after-it-turned-into-racist-nazi/, Microsoft Shut Down its AI Teen Chatbot after it made Racist Tweets} and write a response.
<!--

-->
