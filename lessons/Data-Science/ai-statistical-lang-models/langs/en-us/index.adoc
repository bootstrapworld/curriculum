= Statistical Language Modeling

@description{Students }

@ifproglang{pyret}{
@lesson-prereqs{ds-intro}
}

@ifproglang{codap}{
@lesson-prereqs{ds-intro}
}

@keywords{}

[@lesson-intro-table]
|===
| Lesson Goals
| Students will be able to...

- Define and apply statistical language modeling.
- Explain how predictive text AI relies on probability.

| Student-facing Lesson Goals
|

- Let's explore how probability influences the output of generative AI models.

| Materials
|[.materials-links]
@material-links

|===



== Statistical Language Models

=== Overview

Students explore how text prediction relies on statistical language modeling.

=== Launch

While texting, emailing, or even conducting a web search, you've probably noticed that your thoughts and sentences are sometimes finished for you! Predictive text is a now-common feature of many apps, and it is made possible by a certain form of AI: the statistical language model.

@lesson-instruction{
- Consider this phrase: "What are you"
- On a piece of paper, write down the first "next word" that pops into your mind.
- Write down two additional "next word" choices.
- As a class, let's generate a list of all of the words we developed.
}

@teacher{
Record students' first choice responses on the board. If a word is repeated, keep a tally of how many students suggest each particular word. Highlight the top three choices for the class. Compute the probability of each word choice as a class. So, if there are 25 students in your class, and 8 students chose “doing”, the probability of a student choosing that word is 8/25.
}

@QandA{
- How did you choose your next words?
- Do you think a text messaging app with a predictive text feature would produce the same results as our class?
** Share the image below to see the word options provided by one text messaging app.
}

@right{@image{images/texting-app.png, 200}}

Behind the scenes, the statistical language model takes in a vast corpus of actual text messages. We have already talked about how AI is trained; XXXX

During training, the computer extracts each use of the phrase "What are you" from the training data, along with the subsequent word. It then determines the probability of each possible "next word". In this particular corpus, "up", "doing", and "guys" were most likely to appear after "What are you". As a result, those words are suggested for users to choose from.


=== Investigate

The best way to make sense of statistical language modeling is to try it yourself!

For our corpus, we will use the folk song @handout{old-lady-lyrics.adoc, "There Was an Old Lady Who Swallowed a Fly"}, which tells the nonsensical story of an old lady who swallows a fly, and the unfortunate series of events that follows.

First, we will decompose the title of our corpus into differently sized chunks:

[cols="^.^1,^.^1,<.^8", stripes="none", options="header"]
|===

| n-gram | Quantity			| Decomposition

| 1-gram (unigram)
| 9
| (There) (Was) (an) (Old) (Lady) (Who) (Swallowed) (a) (Fly)

| 2-gram (bigram)
| 8
| (There Was) (Was an) (an Old) (Old Lady) (Lady Who) (Who Swallowed) (Swallowed a) (a Fly)

| 3-gram (trigram)
| 7
| (There Was an) (Was an Old) (an Old Lady) (Old Lady Who) (Lady Who Swallowed) (Who Swallowed a) (Swallowed a Fly)

|===

During statistical language modeling, the computer breaks a text into chunks and then assesses how likely it is that any _single_ word in the text will _follow_ that chunk. We call these chunks @vocab{n-grams}, where @math{n} represents the number of words in the chunk.

Let's dig a little deeper...

@teacher{
To share the song lyrics with students,  have students @handout{old-lady-lyrics.adoc, "read them independently"}, or even listen to a recorded version of the song.
}


The phrase "there was an old lady who swallowed a..." is repeated  in our corpus! Let's zoom in on one unigram from that phrase: “there”.

@QandA{
@Q{Referring to the @handout{old-lady-lyrics.adoc, "lyrics"}: how many times does the word "there" appear in the song?}
@A{4}
@Q{In this corpus, what is the likelihood that the word "there" is followed by the word "was"?: 1/4, 2/4, 3/4, 4/4}
@A{4/4, or 100% probability}
}


In the example you just worked through, you computed the probability that "was" appears after the unigram "there". We can represent the computation you just completed with a special notation:

@math{p(was | there) =}
@math{\frac
	{\mbox{count(there was)}}
	{\mbox{count(there...)}}
= {\frac{4}{4}}}

Put another way: To compute the probability that "was" follows "there", we divide 4 (how many times we see "there was") by 4 (how many times we see "there" followed by anything).


@lesson-instruction{
- Complete @printable-exercise{stat-lang-model-intro.adoc}.
- Be prepared to share and defend your response to the last question on the page.
}


@QandA{
@Q{How will text generated by a _unigram_ statistical language model differ from text generated by a 5-gram model?}
@A{Text produced by a unigram model will be less predictable than text generated by a 5-gram model. It is also more likely to be nonsensical.}
}

During the launch, you decided what word would likely follow "What are you". If your class is like other classes, you probably predicted many of the same words, with "doing" at the top of the list. A big reason that you and your peers came up with similar word lists is because we used a _trigram_.

If asked what word should come after "you", you and your classmates would likely have a much broader range of suggestions. That's because "you" is a unigram.

In the previous activity, the likelihood of one word appearing in a sequence depended only on the word (or words) preceding it. In its most basic form, predictive text AI is a matter of probability!

@teacher{
Are you and your students interested in exploring probability in more depth? Check out @lesson-link{probability-inference} to dig deeper.
}

=== Synthesize

- Which will produce more grammatically correct responses: a 5-gram model or a 2-gram model? Explain.
- Which will produce a more creative response: a 5-gram model or a 2-gram model? Explain.


== Soekia

=== Overview

=== Launch

You've used a paper, pencil, and probability to apply the principles of statistical language modeling. It's time to peek behind the curtain and see how a computer can put this model to use! To make that happen, we're going to explore Soekia, a simplified text generation tool designed for student learning.

@lesson-instruction{
- Go to @link{http://Soekia.ch/GPT/?lang=en}
- Complete the first section of @printable-exercise{soekia-intro.adoc}.
- When you're done, let's do a quick survey: Raise your hand if your story was largely inspired by "Felicia and the Pot of Pinks".
}

@teacher{
The vast majority of students will have a story that is primarily sourced from "Felicia and the Pot of Pinks". On the next section of the worksheet, students will discover exactly _why_ this is the case. Feel free to use this mystery as incentive to move on to the next section of the page!
}

@lesson-instruction{
- Complete the second section of @printable-exercise{soekia-intro.adoc}.
}

@QandA{
@Q{Why were so many of our initial stories all about Felicia and the Pot of Pinks?}
@A{The green bar indicates how closely the document matches the prompt. The story "Felicia and the Pot of Pinks" includes the word "tale" once, "fairy" four times, and the word "me" more than a dozen times. With these frequencies, it is a much closer match to the prompt than any of the other fairy tales.}
}

Let's review what we have done so far:

- We have interacted Soekia's text generation panel. With modern AI, the text generating interface is the only element that we are privy to. Unlike the AI we use daily, Soekia allowed us to glimpse which words and phrases came from which sources.

- We have also peeked at Soekia's documents panel, or corpus. This is a critical feature of all text-generating AI, but ordinarily, it is hidden from us. Soekia also allowed us to see how the level of alignment between each document and the provided prompt.

Let's explore the two remaining panels!

@lesson-instruction{
- Turn to @printable-exercise{soekia-closer-look.adoc}.
- Be prepared to share your responses with the class.
}

@ifnotslide{
@teacher{
As students are working, you can share the three tips, below.
}
}

@ifslide{
Advance to the next slide for student-facing tips on navigating Soekia.
}


@slidebreak

If you feel overwhelmed as you work, here are some tips:

- Click "Pause" to review each of the four panels. Ask yourself, "How is _this_ panel related to each of the other panels, in particular, the _adjacent_ panels?"

- Get curious! *Clicking* is powerful. Each time you click, you access previously hidden information. You can click a document, an N-gram, a suggested word, or even words that appear on the text generation panel.

- To slow down text generation and to allow time to observe changes as they occur, click the "Choose yourself" icon and use your mouse to select words. (You will be prompted to do this in the next activity.)

@teacher{
After they complete the "Closer Look" worksheet, invite students to share out on what they learned. In particular, have students share their predictions and whether they were correct or not. See if, as a class, you can develop an understanding of any unexpected outcomes.
}

=== Investigate

Modern statistical language models like ChatGPT often invite users to adjust the "temperature" of the generated text. For instance, ChatGPT users are encouraged to use a _low_ temperature for tasks that are more focused and less creative tasks. They are encouraged to use a _higher_ temperature for more random and increasingly creative tasks. But why? What does "temperature" actually represent?

@lesson-instruction{
- Turn to @printable-exercise{soekia-temperature.adoc}.
- Pause for class discussion once you have completed the first section.
}

As you discovered, @vocab{temperature} is the parameter that controls the randomness of the model's output as it generates text.

@QandA{
@Q{How would you characterize text generated at a high temperature?}
@Q{How would you characterize text generated at a low temperature?}
@Q{AI sometimes generates false or misleading information. Do you think this is more likely to occur at a high temperature or a low temperature? Explain.}
}


@strategy{AI "Hallucinations"}{

Some experts claim that the term "hallucination" does not accurately capture _why_ generative AI generates incorrect or misleading information. In fact, we encourage you to avoid this term in your classroom.

First, this term attributes intent and consciousness to the AI, giving it human qualities when it is merely executing a program exactly as it is intended to do.

Second, as students have discovered through their interaction with Soekia, all generated output - each and every word, sentence, and paragraph - is nothing more than a hallucination!
}


Let's experiment ...

Chess, tic tac toe, music


=== Synthesize

- A student argues that AI is a reliably correct and credible source of information. How would you respond?
