= Fitting Linear Models

@description{Students learn about model "fitness" through the value _S_ (Standard Deviation of Residuals). They build and fit a variety of linear models to fit a dataset, first by trial-and-error and then using linear regression.}

@lesson-prereqs{linear2-building-models}

@keywords{linear, slope, intercept, slope-intercept, S, Sres, error}

@add-to-lang{fit-model, S}

[@lesson-intro-table]
|===

| Lesson Goals
| Students will be able to...

- Read and interpret real-world data, presented in a scatter plot
- Describe correlations as Strong, Moderate, or Weak
- Model linear relationships using linear functions

| Student-facing Lesson Goals
|

- Let's use Pyret to make predictions with linear models


| Materials
|[.materials-links]
@material-links

| Supplemental Materials
|[.materials-links]
@opt-material-links


| Key Points For The Facilitator
|
-
- Two of the starter files referenced in this file pull data from the same spreadsheet.
  * @starter-file{alg2-states-intro} is used for initial data exploration and the first model. There is no need to have students save this starter file.
  * When we're ready for students to start saving things, we'll refer them to the @starter-file{alg2-states-fit-model}, which has more details in the Definitions Area to save you time.

|===

== Fitting Linear Models @duration{45 minutes}

=== Overview
Students confront the notion of "model fitness". How do we measure how well a model fits? How do we determine which of two models is best? First they'll consider two models for a simple dataset and brainstorm how we could measure which fits better. Then they'll test out their linear models using a new pyret function called `fit-model`, which draws the residuals and computes the Standard Deviation of the Residuals (@math{S}).

=== Launch

In the previous section, we came up with a linear model for the relationship between `pct-college-or-higher` and `median-income`, but it definitely wasn't the best model.

*How do we even measure how good a model is?*

@slidebreak

@lesson-instruction{
- With your partner, complete @printable-exercise{how-could-we-measure-good-fit.adoc}
}

@slidebreak

@QandA{
@Q{What criteria did you come up with for how to assess whether or not a model is a good fit for the data?}
@A{Answers will vary. Ideas might include:}
@A{The points should be as evenly distributed around the model as possible.
- We could see how the number of points above the line and below the line compare.
- We could measure the distance between the points and the line and try to make sure the average distance above is balanced with the average distance below.}

@Q{How could we measure the distance between the data points and the linear model?}
@A{Answers will vary. Ideas might include:}
@A{By drawing vertical lines connecting each data points to the linear model.}
@A{By drawing horizontal lines connecting each data points to the linear model.}
@A{By drawing diagonal lines connecting each data points to the linear model. +
Push students to recognize that in order for this measurement to be useful they would have to be perpendicular to the linear model!}
@A{By drawing squares with one corner on the data point and the opposite corner on the linear model.}
}

@slidebreak

Pyret has a special function called `fit-model` that graphs whatever function we give it on top of a scatter plot of the dataset!

@QandA{
@Q{Take a look at the contract for `fit-model` in your contracts page. +
What is its Domain?}
@A{Like `scatter-plot`, it consumes columns for our __labels__, our @math{x}s, our @math{y}s... *additionally, it __consumes a function*__.}
}

@slidebreak

@lesson-instruction{
- Open the @starter-file{alg2-cheerios} and click "Run" to test out `fit-model` with the dataset and functions you were just looking at.
- What do you Notice? What do you Wonder?
}

@ifnotslide{
[cols="1a,1a", frame="none", grid="none"]
|===
| `fit-model(cheerios-table, "id", "day", "cheerios-on-the-floor", f)`
| `fit-model(cheerios-table, "id", "day", "cheerios-on-the-floor", g)`

| @centered-image{images/cheerios-f.png}
| @centered-image{images/cheerios-g.png}
|===
}

@slidebreak

@right{@image{images/residual.png, 200}}When you graph your model in Pyret, you can see that:

- some of the points are close to the line ("real" @math{y} is close to "predicted" @math{y})
- some points are quite far away ("real" @math{y} is far from "predicted" @math{y})

The difference between any real @math{y} and predicted @math{y} is called the @vocab{residual}, and it measures how far off that one point in the model is from the actual data.

@slidebreak

@QandA{
@Q{There are three terms in the legend at the bottom. What do they refer to?}
@A{The blue line is the model.}
@A{The red dots are the data from the data set.}
@A{Residuals refer to the vertical black lines connecting the data points to the model, representing the distance between the data and the value the model predicts. They vary in length depending on how far above or below the model the data is situated.}

@Q{Compare the `fit-model` display for `f` to the `fit-model` display for `g`. How are they similar? How are they different?}
@A{The x-axis goes from 0 to 10 for both of them.}
@A{The y-axis for `g` stops at 9. It goes up to 20 for `f`.}
@A{Both `f` and `g` have a blue line and red dots.}
@A{`f` has significantly more red dots below the blue line than above it.}
@A{The data points for `g` more or less fill the vertical space of the display, whereas for `f` there are only data points in the bottom half of the display.}

There are @math{S} and @math{R^2} values listed in the top left corner. You probably haven't seen these terms before, but let's see if we can figure out what they mean.

@Q{How do @math{S} and @math{R^2} compare for the two models?}
@A{The values are positive for both models and both @math{S} and @math{R^2} values are smaller for `g` than they are for `f`.}
}

@slidebreak

@teacher{While the remainder of the lesson could be done using the @starter-file{alg2-states-intro}, you will see us refer to @starter-file{alg2-states-fit-model} from here on out. This file contains the same data, but the Definitions Area is set up to save you time. `al-ak` has been predefined and the other @vocab{models} students will be asked to define during the remainder of the lesson have been started for them.

Now is the time to make sure students *Save a Copy* of the file.}

@lesson-instruction{
- Open @starter-file{alg2-states-fit-model} and save a copy that's just for you.
- Complete @printable-exercise{model-college-v-income-2.adoc}.
}

@teacher{Heads up: Sometimes a value has too many digits to be displayed clearly. When this happens, Pyret will convert it to *scientific notation*. While students in an Algebra 2 class will likely have encountered scientific notation before, they may not recognize @math{8.23e5} as @math{8.23 \times 10^5}. You should make sure they understand how to interpret this notation.

@opt{Pyret has a function that will compute @vocab{S} without drawing the graph. This may be useful, especially for students who are struggling with scientific notation: @show{(contract 'S '((t Table) (label String) (xs String) (ys String) (model Number->Number)) "Number")}
}
}

@QandA{
@Q{Based on the @vocab{S} values of the plots you created on this page, what do you think @vocab{S} means?}
@A{Answers will vary, but students should have some sense of the idea that if one model has a lower @vocab{S} value than another model of the same data it indicates a better fit.}
}

@slidebreak
@ifslide{@right{@image{images/residual.png, 200}}}
There are many different tools to calculate the fitness of a model. You may have heard of @math{R}, @math{R^2}, etc...

Statisticians and Data Scientists are careful to use the right tool for the job!

- We want a measure of _error_, so the measure should be zero for a perfect model with no residuals.
- We want a measure that's easy to understand, so in our case it should measure _how many income-dollars of error_ a model has.
- We want a measure that takes the residuals from _every_ data point into account.

@slidebreak

@ifslide{ @right{@image{images/residual.png, 200}} }@vocab{S} is a measure of fitness, which refers to the @vocab{Standard Deviation of the Residuals}.

- The closer the data points are to the model, the smaller the residuals are.
- Smaller residuals mean a smaller @vocab{S}, and a better model!
- We know that if a model fits the data perfectly, the @vocab{S} value would be 0. 
- Unlike other measures of fitness, @vocab{S} is expressed in terms of _units of the y-axis_. An @vocab{S} of `2500` in this dataset means _the standard deviation of the residuals is $2500_ - making it much easier to understand.

@slidebreak

@lesson-point{
The @math{S}-value always has to be considered in the context of the range of values that the model is predicting!
}

A model built from Alaska and Alabama predicts that a 1 percent increase in college degrees is associated with a $5613.67 increase in median household income. 

- The lowest median incomes are found in Mississippi ($39.031), Arkansas ($40,768), and West Virginia ($41,043). 
- The highest median income is found in Maryland ($73,538).

With an S-value of 36165, we know that there’s enough error in the model to predict median incomes that are off by $36,165! That’s enough to double the median income of a state or cut it in half!

@slidebreak

*Compared to the size of the incomes in this dataset, an @vocab{S} value of $36,165 is pretty terrible. __This model should not be trusted!__*

@lesson-instruction{
- Turn to @printable-exercise{s-tells-us.adoc}.
- Consider the @math{S}-value of each model in the context of the range of the data described.
- Decide how well the model is likely to predict values.
}

@QandA{
@Q{Were any of the models described terrific? How do you know?}
@A{Both 2 and 8}
@A{Because the numbers in the range were huge and the @math{S} value was really small.}

@Q{Were any of the models described terrible? How do you know?}
@A{Both 1 and 6}
@A{Because the @math{S}-value was big in comparison to the range.} 
@A{For the first scenario the @math{S}-value was 300, which was the majority of the range between 0 and 400.}
@A{For the sixth scenario, even though the @math{S}-value was only 1, it was much bigger than any of the numbers in the range, which maxed out at two hundredths.}
}

@lesson-instruction{
- Complete @printable-exercise{model-college-v-income-3.adoc}.
- @opt{Complete @opt-printable-exercise{graphing-models.adoc} for a side by side visual comparison of three of your models.}
- What was the best model (lowest @vocab{S}!) you could come up with?
}

@strategy{Going Deeper}{

For a discussion of why the standard error of the regression @math{S} may provide more useful information than @math{R^2}, we recommend visiting @link{https://www.statology.org/standard-error-regression/, this link}.
Further discussion of @vocab{S} and @vocab{Residuals} may be appropriate for older students, or in an AP Statistics class. We also have an entire Bootstrap:Data Science lesson on @lesson-link{standard-deviation}.
}

=== Synthesize

@QandA{
@Q{What does it mean if @math{S} is zero?}
@A{The model fits the data perfectly.}

@Q{Is an @math{S}-value of 1000 bad?}
@A{We have no way of knowing out of context! @math{S}-values only make sense when considered in the context of the range of the dataset! In our income dataset, 1000 is a pretty good @vocab{S}, because $1000 isn't a big margin of error. But in a dataset showing the number of students in a school, 1000 would be a very significant error!}
}

== Finding the Best Linear Model

=== Overview

Students are introduced to a new pyret function called `lr-plot`, which uses linear regression to fit the best possible linear model to the data. 

=== Launch

We've learned how to measure how well linear models fit the data and to decide which linear model does a better job of predicting values, but how do we find the _best possible linear model?_ 

In Statistics, an algorithm called linear regression is used to derive the slope and y-intercept of the best possible model by taking every datapoint into account. Linear regression consumes a dataset and produces a _function_ representing the best linear model.

We could keep guessing and picking two points over and over, and never know if we found the best linear model. Linear regression automatically finds the best-possible model, _for any dataset_. This is pretty amazing!

Pyret's `lr-plot` function finds the best model, and graphs it on top of a scatter plot, and tells us the slope and y-intercept.

@strategy{More `lr-plot` material}{

If you'd like to have students dig deeper into linear regression, there's an @lesson-link{linear-regression, entire lesson} you can use that spends more time interpreting results and writing about findings. This lesson also includes a discussion of @math{R^2}, a different measure of model fitness.
}
=== Investigate

@lesson-instruction{
- Turn to @printable-exercise{interpreting-linear-models.adoc} and complete the first section ("Build a Model Computationally").
- Compare this optimal model to the models you built on @printable-exercise{model-college-v-income-3.adoc}
}

@QandA{
@Q{How close did your models come to the optimal model?}
@Q{Did anything about the optimal model surprise you?}
}

@slidebreak

@lesson-instruction{
Models are only useful if know how to use them!

- Turn to the second section of @printable-exercise{interpreting-linear-models.adoc}.
- Using the interpretation of the `al-ak` model you'll find there as a guide, write up your interpretation of the optimal model you just found for this dataset. Then answer the questions that follow.
- @opt{For more practice, build linear models for *other* relationships in the data. You can use @opt-printable-exercise{building-more-linear-models.adoc}.}
}

@strategy{Optional Activity: Guess the Model!}{

1. Divide students into teams of 2-4, and have each team come up with a linear, real-world scenario, then have them write down a linear function that fits this scenario on a sticky note. Make sure no one else can see the function!
2. On the board or some flip-chart paper, have each team draw a _scatter plot_ for which their linear function is best fit. They should only draw the point cloud - _not the function itself!_ Finally, students title display to describe their real-world scenario (e.g. - "total cost vs. number of tickets purchased").
3. Have teams switch places or rotate, so that each team is in front of another team's scatter plot. Have them figure out the original function, write their best guess on a sticky note, and stick it next to the plot.
4. Have teams return to their original scatter plot, and look at the model their colleagues guessed. How close were they? What strategies did the class use to figure out the model?

- The slope and y-intercepts can be constrained to make the activity easier or harder. For example, limiting these coefficients to whole numbers, positive numbers, etc.
- To extend the activity, have the teams continue rotating so that each group adds their sticky note for the best-guess model. Then do a gallery walk so that students can reflect: were the models all pretty close? All over the place? Were the guesses for one coefficient grouped more tightly than the guesses for another?
}

=== Synthesize

@QandA{
@Q{When does it make sense to make an `lr-plot`?}
@A{When we've identified that the form of the data is linear}
}

- How could we use scatter plots and linear models to find out if taller NBA players tend to make more three-pointers?
- How could we use scatter plots and linear models to find out if wealthier people live longer?
- How could we use scatter plots and linear models to find answers to _other_ questions?

@slidebreak

Our model is built from data about all the existing states, which have college attendance rates between 18.3% (West Virginia) and 52.4% (Washington, DC). Suppose two new states were to join the union, one with a 30% college attendance rate and the other with a 90% attendance rate.

Is our model more reliable for one of these states than another? Why or why not?

@teacher{A model is only as good as the data it was based on. With lots of data between 18.3-52%, this model is much more reliable for the 30% state than the 90% one!}
