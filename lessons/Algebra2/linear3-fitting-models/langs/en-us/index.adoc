= Fitting Linear Models

@description{Students learn to gauge model "fitness" using _S_ value (Standard Deviation of Residuals), building and fitting a variety of linear models to a dataset, first by trial-and-error and then using linear regression.}

@lesson-prereqs{linear2-building-models, fitting-models}

@keywords{linear, slope, intercept, slope-intercept, S, Sres, error}

@add-to-lang{fit-model, S}

[@lesson-intro-table]
|===

| Lesson Goals
| Students will be able to...
@objectives

@scrub{
- Read and interpret real-world data, presented in a scatter plot
- Describe correlations as Strong, Moderate, or Weak
- Model linear relationships using linear functions
}

| Student-facing Lesson Goals
|

- Let's use Pyret to fit linear models to our dataset.


| Materials
|[.materials-links]
@material-links

| Supplemental Materials
|[.materials-links]
@opt-material-links

| Preparation
| @preparation{
- Decide whether you plan to: 
  * Launch this lesson with @opt-printable-exercise{review.adoc}
  * Give students more practice with defining and refining models using @opt-printable-exercise{model-college-v-income-3.adoc}
- If yes, you'll need to make copies and/or share links. If you are using our Google Slides, you may also want to adjust them accordingly.
}

|===

== Fitting and Comparing Linear Models
@objective{model-interpret-residuals}
@objective{model-predictions}
@objective{model-fit-function}

=== Overview

Students compare the fits of the Alabama-Alaska model and the Michigan-California model. They also have the option of building a Massachusetts-Nevada model and trying to make some better models of their own through trial and error.

=== Launch

@teacher{
@opt{
@opt-printable-exercise{review.adoc} circles back to what students learned in @lesson-link{fitting-models} so their understanding of @math{S}-values and @math{\text{residuals}} is fresh.
}
}

@lesson-instruction{
- Open your copy of @starter-file{alg2-states} and click "Run".
- Turn to @printable-exercise{model-college-v-income-2.adoc} and complete the first section.
}

@slidebreak

@center{@image{images/al-ak.png, 450}}

@QandA{
@Q{What does it mean that the @math{S}-value for our `al-ak` model is 36165?}
@A{We know that thereâ€™s enough error in the @vocab{model} to predict median incomes that are off by $36,165!}
@A{We have to look at the @vocab{range of the dataset} to really know what the @math{S}-value means!}
@A{Considering the range of the data, the error in the model is enough to double the median income of a state or cut it in half!
 *** The lowest median incomes are found in Mississippi ($39.031), Arkansas ($40,768), and West Virginia ($41,043).
 *** The highest median income is found in Maryland ($73,538).
}
}

@slidebreak

Compared to the size of the incomes in this dataset, an @vocab{S} value of $36,165 is pretty terrible. 
__This model should not be trusted!__

@opt-block{
@teacher{
Give students a chance to adjust their `al-ak` model by trial and error and practice defining a second model using Massachusetts and Nevada by having them complete @opt-printable-exercise{model-college-v-income-3.adoc}.
}

@slidebreak

@QandA{
@Q{We made a model using Alabama and Alaska. The @math{S}-value for this `al-ak` model is ~36164.683.}
@Q{The `ma-nv` model was made using Massachusetts and New Mexico. The @math{S}-value for the `ma-nv` model is ~7504.54.}
@Q{What comparisons can we make between these two models?}
@A{We expect significantly less error in predictions made from the `ma-nv` model.}
@A{The @math{S}-value of the `ma-nv` is 28660 dollars less!}
@A{We expect predictions made with the `ma-nv` model to 79% less error than predictions made with the `al-ak` model.}
}
}

=== Investigate

Can we do better?

@lesson-instruction{
- Find the `mi-ca` model in the Definitions Area of the @starter-file{alg2-states}. It was built using data from Michigan and California.
- Complete the second section: *Comparing Models* on @printable-exercise{model-college-v-income-2.adoc} to determine how the `mi-ca` model compares to the `al-ak` model. 
}

@slidebreak

@QandA{
@Q{The @math{S}-value for the `al-ak` model (made using data points for Alabama and Alaska) is ~36164.683.}
@Q{The @math{S}-value for the `mi-ca` model (made using data points for Michigan and California) is ~10779.923.}
@Q{What comparisons can we make between these two models?}
@A{We expect significantly less error in predictions made from the `mi-ca` model.}
@A{Based on the @math{S}-values, we expect prediction made with `mi-ca` to have about 25385 dollars less error than those made with the `al-ak` model!}
@A{We expect predictions made with the `mi-ca` model to have about 70% less error than predictions made with the `al-ak` model.}
}

@teacher{
Make sure your students know to how to calculate what percent less error we expect in predictions from the `mi-ca` model than the `al-ak` model. Students are about to calculate what percent more or less error we expect from predictions made with their models, which will all have different @math{S}-values. Right now everyone is looking at the same @math{S}-values so supporting them will be much less work!

@indented{
Percent Change &#8192; = &#8192;
@math{
\frac{&#8192; \text{Difference} &#8192; \text{between} &#8192; \text{the} &#8192; \text{S-values}&#8192;}
{\text{S-value} &#8192; \text{for} &#8192; \text{al-ak} &#8192; \text{model}}
\times 100 &#8192; = &#8192; 
\frac{25384.76}{36164.683} = .70  &#8192; &rarr; &#8192;  70 \%}
}
}

@lesson-instruction{
- Return to @printable-exercise{model-college-v-income-2.adoc} and complete the third section: *A Model of Your Own*, using two states that you identify as likely to produce a model with a better fit.
}

@teacher{
@opt{For a side by side visual comparison of their models have students complete @opt-printable-exercise{graphing-models.adoc}. Students who did not complete @opt-printable-exercise{model-college-v-income-3.adoc} should just sketch the two models they made.
}
}
 
@slidebreak

@QandA{
@Q{What was the best model (lowest @vocab{S}!) you could come up with?}
@Q{How did your model compare to the `al-ak` model?}
@Q{When we compared the @math{\text{S-values}} of the models, why did we divide by the @math{\text{S-value}} from the `al-ak` model?}
@A{Because we were asking how much more or less error we expected in predictions made with our model than with the `al-ak` model - we were comparing to the `al-ak` model.}
@A{If we had divide the `change in S` by the @math{\text{S-value}} of our model it would have answered a different question: "How much more or less error do we expect in predictions made with the `al-ak` model than with our model?"}
}

@strategy{Going Deeper}{

For a discussion of why the standard error of the regression @math{S} may provide more useful information than @math{R^2}, we recommend visiting @link{https://www.statology.org/standard-error-regression/, this link}.
Further discussion of @vocab{S} and @vocab{Residuals} may be appropriate for older students, or in an AP Statistics class. We also have an entire Bootstrap:Data Science lesson on @lesson-link{standard-deviation}.
}

=== Synthesize

@QandA{
@Q{What does it mean if @math{S} is zero?}
@A{The model fits the data perfectly.}

@Q{Is an @math{S}-value of 1000 bad?}
@A{Without more context, we have no way of knowing! @math{S}-values only make sense when considered in the alongside the range of the dataset. In our income dataset, 1000 is a pretty good @vocab{S}, because $1000 isn't a big margin of error. But in a dataset showing the number of students in a school, 1000 would be a very significant error!}
}

== Finding the Best Linear Model
@objective{model-interpret-residuals}
@objective{linear-regression}

=== Overview

Students are introduced to the `lr-plot` function in Pyret, which uses linear regression to fit the best possible linear model to the data.

@teacher{
If you want to spend more time with students interpreting regression results, writing about findings, or digging into @math{R^2} (a different measure of model fitness), we have an entire @lesson-link{linear-regression, Bootstrap: Data Science lesson on Linear Regression}.
}

=== Launch

We've learned how to measure how well linear models fit the data and to decide which linear model does a better job of predicting values. We could keep guessing and picking two points over and over, and our models would likely improve, but we'd never know whether we had found the _best possible linear model_.

Luckily statisticians have developed an algorithm called @vocab{linear regression}, which, given any dataset, considers every point and produces the best possible linear model, known as the @vocab{line of best fit}.

@slidebreak

Pyret's `lr-plot` function uses linear regression to graph the best possible linear model on top of a scatter plot of the dataset, and tell us the slope, y-intercept and @vocab{S-value} of the model.


=== Investigate

Let's use Pyret to find the best possible linear model for predicting median income of a state from the percent of the population that has attended college.

@lesson-instruction{
- Turn to @printable-exercise{interpreting-linear-models.adoc} and complete the first section ("Build a Model Computationally").
- Compare this optimal model to the models you built on @printable-exercise{model-college-v-income-2.adoc}
- @opt{If you completed @opt-printable-exercise{model-college-v-income-3.adoc}, compare the model on this page as well!}
}

@slidebreak

@center{@image{images/lr-plot-college-v-income.png, 450}}

@QandA{
@Q{How close did your models come to the optimal model?}
@Q{Did anything about the optimal model surprise you?}
}

=== Synthesize

@QandA{
@Q{Why is it advantageous to use linear regression to find a model?}
@A{Instead of focusing on two points, linear regression considers *all* of the points!}
@A{We know that we are working with the best possible linear model.}
}

== Using and Interpreting our Models

=== Overview

Students interpret their models, practice using them to make predictions, and consider what range of inputs will yield more reliable predictions.

=== Launch
Models are only useful if we know how to use and interpret them!

@lesson-instruction{
- Find the second section of @printable-exercise{interpreting-linear-models.adoc}: *Interpreting the `al-ak` model*.
- Read the model interpretation with your partner and identify where the information on each of the fill in the blanks comes from.
- Then answer the question.
}

@slidebreak

@QandA{
@Q{How could we use the model to predict the median income for a state with a 30% college attendance rate?}
@A{Compute `al-ak(30)` by substituting 30 into the equation for @math{x}.}
@A{@math{5614 \times 30 + 83616 = ~252306}}
}

=== Investigate


@lesson-instruction{
- Turn to the third section of @printable-exercise{interpreting-linear-models.adoc}.
- Using the interpretation of the `al-ak` model as a guide, write up your interpretation of the optimal model you just found for this dataset. Then answer the questions that follow.
}

@teacher{
@opt{For more practice, have students choose two other columns in the dataset to explore the relationship between and build linear models for using @opt-printable-exercise{building-more-linear-models.adoc}.}
}

=== Synthesize

@QandA{
@Q{When does it make sense to make an `lr-plot`?}
@A{When we've identified that the form of the data is linear}

@Q{Our model is built from data about all of the existing states. College attendance rates range from 18.3% (West Virginia) to 52.4% (Washington, DC). +
Suppose two new states were to join the union, one with a 30% college attendance rate and the other with a 90% attendance rate. Is our model more reliable for one of these states than another? Why or why not?
}
@A{This model is much more reliable for the 30% state than the 90% one!}
@A{A model is only as good as the data it was based on and the data in this dataset ranges from 18.3% to 52%.}

@Q{If we could remove any row from this dataset to make our line fit better, which would you remove?}
@A{Washington, D.C. - it's an outlier in virtually every measure!}

@Q{Is it fair to remove that row? Why or why not?}
@A{Reason why: Washington, D.C. is a major metropolitan area! You can just erase those people to make the line *fit* better!}
@A{Reason why not: Washington, D.C. is not representative of the rest of the country at all. The unusual concentration of highly-educated people working for lower income is a special case because of all the government employees. Therefore, it's ok to remove.}

@Q{How could we use scatter plots and linear models to find answers to _other_ questions, for example:
  * Do taller NBA players tend to make more three-pointers?
  * Do wealthier people live longer?
}
@A{Find a dataset that contains the explanatory variable and response variable, import it into Pyret, and build an lr-plot!}
}

@ifnotslide{
@strategy{Optional Activity: Guess the Model!}{

1. Divide students into small groups (2-4), and have each team come up with a linear, real-world scenario, then have them write down a linear function that fits this scenario on a sticky note. Make sure no one else can see the function!
2. On the board or some flip-chart paper, have each team draw a _scatter plot_ for which their linear function is best fit. They should only draw the point cloud - _not the function itself!_ Finally, students title their scatter plot to describe their real-world scenario (e.g. - "total cost vs. number of tickets purchased").
3. Have teams rotate so that each team is in front of another team's scatter plot. Have them figure out the original function, write their best guess on a sticky note, and stick it next to the plot.
4. Have teams return to their original scatter plot, and look at the model their colleagues guessed. How close were they? What strategies did the class use to figure out the model?

- The slope and y-intercepts can be constrained to make the activity easier or harder. For example, limiting these model settings to whole numbers, positive numbers, etc.
- To extend the activity, have the teams continue rotating so that each group adds their sticky note for the best-guess model. Then do a gallery walk so that students can reflect: were the models all pretty close? All over the place? Were the guesses for one model setting more tightly than the guesses for another?
}
}
